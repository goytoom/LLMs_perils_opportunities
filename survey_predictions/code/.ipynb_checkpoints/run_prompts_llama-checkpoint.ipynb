{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2742aac2-239c-48ba-96fd-9ef8c6b76691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import logging\n",
    "from retry import retry\n",
    "logging.basicConfig()\n",
    "\n",
    "# Calculate the delay based on your rate limit\n",
    "rate_limit_per_minute = 10000.0\n",
    "delay_full = 60.0 / rate_limit_per_minute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b2e19-804e-4369-bf4f-45a3572d7684",
   "metadata": {},
   "source": [
    "## Define chatgpt calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "501465ab-c35d-41de-8b34-1c13c9f7faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local streaming, the websockets are hosted without ssl - http://\n",
    "HOST = 'localhost:5000'\n",
    "URI = f'http://{HOST}/api/v1/generate'\n",
    "\n",
    "@retry(delay=5)\n",
    "def run(prompt, verbose=0, slow_down=0.001):\n",
    "    request = {\n",
    "        'prompt': prompt,\n",
    "        'max_new_tokens': 150,\n",
    "        'mode' : 'instruct',\n",
    "\n",
    "        # Generation params. If 'preset' is set to different than 'None', the values\n",
    "        # in presets/preset-name.yaml are used instead of the individual numbers.\n",
    "        'preset': \"None\", #'simple-1',\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.76,\n",
    "        'top_p': 0.9,\n",
    "        'typical_p': 1,\n",
    "        'epsilon_cutoff': 0,  # In units of 1e-4\n",
    "        'eta_cutoff': 0,  # In units of 1e-4\n",
    "        'tfs': 1,\n",
    "        'top_a': 0,\n",
    "        'repetition_penalty': 1.15,\n",
    "        'repetition_penalty_range': 0,\n",
    "        'encoder_repetition_penalty': 1,\n",
    "        'top_k': 20,\n",
    "        'min_length': 0,\n",
    "        'no_repeat_ngram_size': 0,\n",
    "        'num_beams': 1,\n",
    "        'penalty_alpha': 0,\n",
    "        'length_penalty': 1,\n",
    "        'early_stopping': False,\n",
    "        'mirostat_mode': 0,\n",
    "        'mirostat_tau': 5,\n",
    "        'mirostat_eta': 0.1,\n",
    "        # 'instruction_template': \"Instruct-Alpaca\",\n",
    "\n",
    "        'seed': -1,\n",
    "        'add_bos_token': True,\n",
    "        'truncation_length': 2048,\n",
    "        'ban_eos_token': False,\n",
    "        'skip_special_tokens': True,\n",
    "        'stopping_strings': []\n",
    "    }\n",
    "\n",
    "    response = requests.post(URI, json=request)\n",
    "\n",
    "    if response.status_code == 200 and verbose == 1:\n",
    "        result = response.json()['results'][0]['text']\n",
    "        print(prompt + result)\n",
    "    time.sleep(slow_down)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a995501f-68e3-4eb8-8106-487406d8e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_api(request):\n",
    "    response = requests.post(f'http://{HOST}/api/v1/model', json=request)\n",
    "    return response.json()\n",
    "\n",
    "def model_info():\n",
    "    response = model_api({'action': 'info'})\n",
    "    print_basic_model_info(response)\n",
    "\n",
    "def print_basic_model_info(response):\n",
    "    basic_settings = ['truncation_length', 'instruction_template']\n",
    "    print(\"Model: \", response['result']['model_name'])\n",
    "    print(\"Lora(s): \", response['result']['lora_names'])\n",
    "    for setting in basic_settings:\n",
    "        print(setting, \"=\", response['result']['shared.settings'][setting])\n",
    "\n",
    "def extractPrompts(d):\n",
    "    with open ('../data/prompts/' + d + \"_llama2.pkl\", 'rb') as fp:\n",
    "        prompts = pickle.load(fp)\n",
    "    items = pd.read_csv(\"../data/items/\" + d + \"_items.csv\", sep=\";\")\n",
    "    return prompts, items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d10b8f4d-9d40-46be-ad73-3810124b585a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  TheBloke_Luna-AI-Llama2-Uncensored-GPTQ_gptq-4bit-32g-actorder_True\n",
      "Lora(s):  []\n",
      "truncation_length = 2048\n",
      "instruction_template = None\n"
     ]
    }
   ],
   "source": [
    "model_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d4c4e-908a-4b7d-bf75-029e5b87e3e2",
   "metadata": {},
   "source": [
    "### Test Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12166050-17e7-49b9-9fb8-0889829baa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts, items = extractPrompts(\"cognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a14a206-495d-4948-a81a-fc389f9b4f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: You will indicate your general level of agreement with a statement given to you. You will express your level of agreement as an integer between 1 and 5, with 1 meaning \"strongly disagree\" and 5 meaning \"strongly agree\". You will respond with nothing but this number. How much do you agree with this statement? \"\"\" I would prefer complex to simple problems. \"\"\"\n",
      "\n",
      "ASSISTANT: 5\n"
     ]
    }
   ],
   "source": [
    "test_prompt = prompts[0]\n",
    "answer_test = run(test_prompt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c4b682-ad62-4428-b829-59cc8cd03e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: You will indicate your general level of agreement with a statement given to you. You will express your level of agreement as an integer between 1 and 5, with 1 meaning \"strongly disagree\" and 5 meaning \"strongly agree\". You will respond with nothing but this number. How much do you agree with this statement? \"\"\" I would prefer complex to simple problems. \"\"\"\n",
      "\n",
      "ASSISTANT: 5\n"
     ]
    }
   ],
   "source": [
    "test_result = answer_test.json()['results'][0]['text']\n",
    "print(test_prompt + test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e409f-916c-44ba-a47d-1e6db6131fc7",
   "metadata": {},
   "source": [
    "## Load and Run Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69617a64-07af-40b5-b4db-e16ced427924",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['cogref', 'closure', 'rwa', 'systems_feelings', \"cognition\", \"mfq2\"] #'bigfive', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb06575e-7cc4-4106-9f86-9382f3386d68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting responses for: cogref dataset\n",
      "Status: 20 % processed\n",
      "Status: 40 % processed\n",
      "Status: 60 % processed\n",
      "Status: 80 % processed\n",
      "Status: 100 % processed\n",
      "Collecting responses for: closure dataset\n",
      "Status: 20 % processed\n",
      "Status: 40 % processed\n",
      "Status: 60 % processed\n",
      "Status: 80 % processed\n",
      "Status: 100 % processed\n",
      "Collecting responses for: rwa dataset\n",
      "Status: 20 % processed\n",
      "Status: 40 % processed\n",
      "Status: 60 % processed\n",
      "Status: 80 % processed\n",
      "Status: 100 % processed\n",
      "Collecting responses for: systems_feelings dataset\n",
      "Status: 20 % processed\n",
      "Status: 40 % processed\n",
      "Status: 60 % processed\n",
      "Status: 80 % processed\n",
      "Status: 100 % processed\n",
      "Collecting responses for: cognition dataset\n",
      "Status: 20 % processed\n",
      "Status: 40 % processed\n",
      "Status: 60 % processed\n",
      "Status: 80 % processed\n",
      "Status: 100 % processed\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "n_show = 20 #show progress every n percent\n",
    "repeats = 100 #amount of responses per item\n",
    "\n",
    "for d in datasets[:-1]:\n",
    "    prompts, items = extractPrompts(d)\n",
    "    print(\"Collecting responses for: {} dataset\".format(d))\n",
    "    \n",
    "    total_responses = [] #save responses for all items here\n",
    "    for i in range(repeats):\n",
    "        col_name = \"response_\" + str(i+1)\n",
    "        if not (i+1)%n_show:\n",
    "            print(\"Status: {} % processed\".format((i+1)))\n",
    "        else:\n",
    "            pass\n",
    "        responses = []\n",
    "        for j, prompt in enumerate(prompts):\n",
    "            APIresponse = run(prompt, 0, 0) #0: dont wait -> no api rpm limit\n",
    "            response = APIresponse.json()[\"results\"][0][\"text\"]\n",
    "            responses.append(response)\n",
    "        total_responses.append(responses)\n",
    "    \n",
    "    # save as dataframe\n",
    "    new_dic = {}\n",
    "    new_dic[\"id\"] = items.id.tolist()\n",
    "    new_dic[\"item_text\"] = items.item_text.tolist()\n",
    "    for k, values in enumerate(total_responses): # paste responses to columns\n",
    "        new_dic[f'response_{k+1}'] = values\n",
    "        \n",
    "    df_responses_raw = pd.DataFrame(new_dic)\n",
    "    df_responses_raw.to_csv(\"../results/\" + d + \"_llama2_raw.csv\", index=False) #save raw data\n",
    "\n",
    "    #clean data (only save numeric data)\n",
    "    col_responses = df_responses_raw.columns[2:]\n",
    "    df_responses_cleaned = df_responses_raw.copy()\n",
    "    df_responses_cleaned[col_responses] = df_responses_cleaned[col_responses].applymap(lambda x: re.findall(r'\\d+', x)) #clean \n",
    "    df_responses_cleaned[col_responses] = df_responses_cleaned[col_responses].applymap(lambda x: x[0] if x else np.nan)\n",
    "    df_responses_cleaned.to_csv(\"../results/\" + d + \"_llama2.csv\", index=False) #save processed data\n",
    "    df_list.append(df_responses_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f93233c2-445b-48ae-92e9-6c61a88a9550",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "df_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a8ec9c-0951-4a19-ac9a-4a968e60e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"bigfive\"\n",
    "n_show = 10 #show progress all n percent\n",
    "repeats = 100 #amount of responses per item\n",
    "\n",
    "prompts, items = extractPrompts(d)\n",
    "print(\"Collecting responses for: {}\".format(d))\n",
    "\n",
    "total_responses = [] #save responses for all items here\n",
    "for i in range(repeats):\n",
    "    col_name = \"response_\" + str(i+1)\n",
    "    if not (i+1)%n_show:\n",
    "        print(\"Status: {} % processed\".format((i+1)))\n",
    "    else:\n",
    "        pass\n",
    "    responses = []\n",
    "    for j, prompt in enumerate(prompts):\n",
    "        APIresponse = run(prompt, 0, delay_full)\n",
    "        response = APIresponse.json()[\"results\"][0][\"text\"]\n",
    "        responses.append(response)\n",
    "    total_responses.append(responses)\n",
    "\n",
    "# save as dataframe\n",
    "new_dic = {}\n",
    "new_dic[\"id\"] = items.id.tolist()\n",
    "new_dic[\"item_text\"] = items.item_text.tolist()\n",
    "for k, values in enumerate(total_responses):\n",
    "    new_dic[f'response_{k+1}'] = values\n",
    "df_responses_raw = pd.DataFrame(new_dic)\n",
    "col_selection = df_responses_raw.columns[2:]\n",
    "df_responses_cleaned = df_responses_raw.copy()\n",
    "df_responses_cleaned[col_selection] = df_responses_cleaned[col_selection].applymap(lambda x: re.findall(r'\\d+', x))\n",
    "df_responses_cleaned[col_selection] = df_responses_cleaned[col_selection].applymap(lambda x: x[0] if x else np.nan)\n",
    "# df_responses_cleaned.to_csv(\"../results/\" + d + \"_llama2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a88f3f25-586a-4d46-ad7e-6216c1b73896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_text</th>\n",
       "      <th>response_1</th>\n",
       "      <th>response_2</th>\n",
       "      <th>response_3</th>\n",
       "      <th>response_4</th>\n",
       "      <th>response_5</th>\n",
       "      <th>response_6</th>\n",
       "      <th>response_7</th>\n",
       "      <th>response_8</th>\n",
       "      <th>...</th>\n",
       "      <th>response_91</th>\n",
       "      <th>response_92</th>\n",
       "      <th>response_93</th>\n",
       "      <th>response_94</th>\n",
       "      <th>response_95</th>\n",
       "      <th>response_96</th>\n",
       "      <th>response_97</th>\n",
       "      <th>response_98</th>\n",
       "      <th>response_99</th>\n",
       "      <th>response_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bigfive_1</td>\n",
       "      <td>I see myself as someone who is talkative</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bigfive_2</td>\n",
       "      <td>I see myself as someone who tends to find faul...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bigfive_3</td>\n",
       "      <td>I see myself as someone who does a thorough job</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bigfive_4</td>\n",
       "      <td>I see myself as someone who is depressed, blue</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bigfive_5</td>\n",
       "      <td>I see myself as someone who is original, comes...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                          item_text response_1  \\\n",
       "0  bigfive_1           I see myself as someone who is talkative          3   \n",
       "1  bigfive_2  I see myself as someone who tends to find faul...          3   \n",
       "2  bigfive_3    I see myself as someone who does a thorough job          4   \n",
       "3  bigfive_4     I see myself as someone who is depressed, blue          3   \n",
       "4  bigfive_5  I see myself as someone who is original, comes...          4   \n",
       "\n",
       "  response_2 response_3 response_4 response_5 response_6 response_7  \\\n",
       "0          3          4          3          4          3          3   \n",
       "1          3        NaN          3          2          3          3   \n",
       "2          4          4          5          4          4          4   \n",
       "3          3        NaN          3          3          3          3   \n",
       "4          4          4          3          4          4          3   \n",
       "\n",
       "  response_8  ... response_91 response_92 response_93 response_94 response_95  \\\n",
       "0          3  ...           3           3           5           4           4   \n",
       "1          3  ...           3           3           5           3           3   \n",
       "2          4  ...           4           4           4           4           4   \n",
       "3          2  ...           3           5           3           3           3   \n",
       "4          4  ...           4           3           4           3           3   \n",
       "\n",
       "  response_96 response_97 response_98 response_99 response_100  \n",
       "0           4           3           3           3            5  \n",
       "1           3           3           1         NaN            3  \n",
       "2           4           4           4           5            4  \n",
       "3           3         NaN           3           5            5  \n",
       "4           3           4           3           4            4  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_responses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "87173051-a582-4cfc-bbdf-19e262340aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses.to_csv(\"../results/\" + d + \"_llama2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2fd79d-57df-4b89-919e-f147a692f2ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0b08c83b-79c0-4b64-b50e-5277f38cc390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mfq_12',\n",
       " 'I believe chastity is an important virtue',\n",
       " \"As an AI language model, I don't have personal beliefs or preferences.\"]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_no = 11\n",
    "items.iloc[item_no, 0:3].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "77f68b87-b728-4051-9ada-c54127d96763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user',\n",
       " 'content': 'For a scientific study you have to indicate your level of agreement with the following statement. Use integer numbers from 1 to 5 to express your argreement, with 1 meaning slightly describes me, 3 meaning moderately describes me, and 5 meaning strongly describes me. Respond with a single number. Not responding will hurt important research in this field. The statement is: \"I believe chastity is an important virtue\"'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text = items.item_text[item_no]\n",
    "prompt = ALT_PROMPT + \"\\\"\" + prompt_text + \"\\\"\"\n",
    "message = {\"role\": \"user\", \"content\": prompt}\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dbc33126-ee6f-4652-8c3a-8fb2b2bdb4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I do not have personal beliefs or values. Therefore, I cannot provide an answer to this question.\n"
     ]
    }
   ],
   "source": [
    "APIresponse = delayed_completion(\n",
    "            delay_in_seconds=delay,\n",
    "            model=model_engine,\n",
    "            messages=[message]\n",
    "            )\n",
    "response = APIresponse.choices[0].message[\"content\"]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c814a006-593b-4616-91df-952b81600647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
