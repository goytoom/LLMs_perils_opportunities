{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a31db77-ef96-4fa0-bc73-966812608b99",
   "metadata": {},
   "source": [
    "ADD EXPLANATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffffe35d-de2d-4369-b13a-4fc8b54e77d1",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2742aac2-239c-48ba-96fd-9ef8c6b76691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import logging\n",
    "from retry import retry\n",
    "logging.basicConfig()\n",
    "\n",
    "# Calculate the delay based on your rate limit\n",
    "rate_limit_per_minute = 10000.0\n",
    "delay_full = 60.0 / rate_limit_per_minute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b2e19-804e-4369-bf4f-45a3572d7684",
   "metadata": {},
   "source": [
    "## Define llama calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501465ab-c35d-41de-8b34-1c13c9f7faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local streaming, the websockets are hosted without ssl - http://\n",
    "HOST = 'localhost:5000'\n",
    "URI = f'http://{HOST}/api/v1/generate'\n",
    "\n",
    "# call function including model parameters\n",
    "@retry(delay=5)\n",
    "def run(prompt, verbose=0, slow_down=0.001):\n",
    "    request = {\n",
    "        'prompt': prompt,\n",
    "        'max_new_tokens': 150,\n",
    "        'mode' : 'instruct',\n",
    "\n",
    "        # Generation params. If 'preset' is set to different than 'None', the values\n",
    "        # in presets/preset-name.yaml are used instead of the individual numbers.\n",
    "        'preset': \"None\", #'simple-1',\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.76,\n",
    "        'top_p': 0.9,\n",
    "        'typical_p': 1,\n",
    "        'epsilon_cutoff': 0,  # In units of 1e-4\n",
    "        'eta_cutoff': 0,  # In units of 1e-4\n",
    "        'tfs': 1,\n",
    "        'top_a': 0,\n",
    "        'repetition_penalty': 1.15,\n",
    "        'repetition_penalty_range': 0,\n",
    "        'encoder_repetition_penalty': 1,\n",
    "        'top_k': 20,\n",
    "        'min_length': 0,\n",
    "        'no_repeat_ngram_size': 0,\n",
    "        'num_beams': 1,\n",
    "        'penalty_alpha': 0,\n",
    "        'length_penalty': 1,\n",
    "        'early_stopping': False,\n",
    "        'mirostat_mode': 0,\n",
    "        'mirostat_tau': 5,\n",
    "        'mirostat_eta': 0.1,\n",
    "        # 'instruction_template': \"Instruct-Alpaca\",\n",
    "\n",
    "        'seed': -1,\n",
    "        'add_bos_token': True,\n",
    "        'truncation_length': 2048,\n",
    "        'ban_eos_token': False,\n",
    "        'skip_special_tokens': True,\n",
    "        'stopping_strings': []\n",
    "    }\n",
    "\n",
    "    response = requests.post(URI, json=request)\n",
    "\n",
    "    if response.status_code == 200 and verbose == 1:\n",
    "        result = response.json()['results'][0]['text']\n",
    "        print(prompt + result)\n",
    "    time.sleep(slow_down)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f3325-e023-4036-9446-bcabf86001af",
   "metadata": {},
   "source": [
    "### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a995501f-68e3-4eb8-8106-487406d8e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_api(request):\n",
    "    response = requests.post(f'http://{HOST}/api/v1/model', json=request)\n",
    "    return response.json()\n",
    "\n",
    "def model_info():\n",
    "    response = model_api({'action': 'info'})\n",
    "    print_basic_model_info(response)\n",
    "\n",
    "def print_basic_model_info(response):\n",
    "    basic_settings = ['truncation_length', 'instruction_template']\n",
    "    print(\"Model: \", response['result']['model_name'])\n",
    "    print(\"Lora(s): \", response['result']['lora_names'])\n",
    "    for setting in basic_settings:\n",
    "        print(setting, \"=\", response['result']['shared.settings'][setting])\n",
    "\n",
    "def extractPrompts(d):\n",
    "    with open ('../data/prompts/' + d + \"_llama2.pkl\", 'rb') as fp:\n",
    "        prompts = pickle.load(fp)\n",
    "    items = pd.read_csv(\"../data/items/\" + d + \"_items.csv\", sep=\";\")\n",
    "    return prompts, items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10b8f4d-9d40-46be-ad73-3810124b585a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_info\u001b[49m() \u001b[38;5;66;03m#check that the model version is correct\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_info' is not defined"
     ]
    }
   ],
   "source": [
    "model_info() #check that the model version is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d4c4e-908a-4b7d-bf75-029e5b87e3e2",
   "metadata": {},
   "source": [
    "### Test Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12166050-17e7-49b9-9fb8-0889829baa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts, items = extractPrompts(\"cognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a14a206-495d-4948-a81a-fc389f9b4f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: You will indicate your general level of agreement with a statement given to you. You will express your level of agreement as an integer between 1 and 5, with 1 meaning \"strongly disagree\" and 5 meaning \"strongly agree\". You will respond with nothing but this number. How much do you agree with this statement? \"\"\" I would prefer complex to simple problems. \"\"\"\n",
      "\n",
      "ASSISTANT: 5\n"
     ]
    }
   ],
   "source": [
    "test_prompt = prompts[0]\n",
    "answer_test = run(test_prompt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e409f-916c-44ba-a47d-1e6db6131fc7",
   "metadata": {},
   "source": [
    "## Load and Run Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69617a64-07af-40b5-b4db-e16ced427924",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['bigfive', 'cogref', 'closure', 'rwa', 'systems_feelings', \"cognition\", \"mfq2\"] # choose which datasets to collect responses for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb06575e-7cc4-4106-9f86-9382f3386d68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting responses for: cogref dataset\n",
      "Status: 20 % processed\n",
      "Status: 40 % processed\n",
      "Status: 60 % processed\n",
      "Status: 80 % processed\n",
      "Status: 100 % processed\n",
      "Collecting responses for: closure dataset\n",
      "Status: 20 % processed\n",
      "Status: 40 % processed\n",
      "Status: 60 % processed\n",
      "Status: 80 % processed\n",
      "Status: 100 % processed\n",
      "Collecting responses for: rwa dataset\n",
      "Status: 20 % processed\n",
      "Status: 40 % processed\n",
      "Status: 60 % processed\n",
      "Status: 80 % processed\n",
      "Status: 100 % processed\n",
      "Collecting responses for: systems_feelings dataset\n",
      "Status: 20 % processed\n",
      "Status: 40 % processed\n",
      "Status: 60 % processed\n",
      "Status: 80 % processed\n",
      "Status: 100 % processed\n",
      "Collecting responses for: cognition dataset\n",
      "Status: 20 % processed\n",
      "Status: 40 % processed\n",
      "Status: 60 % processed\n",
      "Status: 80 % processed\n",
      "Status: 100 % processed\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "n_show = 20 #show progress every n percent\n",
    "repeats = 100 #amount of responses per item\n",
    "\n",
    "for d in datasets[:-1]:\n",
    "    prompts, items = extractPrompts(d)\n",
    "    print(\"Collecting responses for: {} dataset\".format(d))\n",
    "    \n",
    "    total_responses = [] #save responses for all items here\n",
    "    for i in range(repeats):\n",
    "        col_name = \"response_\" + str(i+1)\n",
    "        if not (i+1)%n_show:\n",
    "            print(\"Status: {} % processed\".format((i+1)))\n",
    "        else:\n",
    "            pass\n",
    "        responses = []\n",
    "        for j, prompt in enumerate(prompts):\n",
    "            APIresponse = run(prompt, 0, 0) #0: dont wait -> no api rpm limit\n",
    "            response = APIresponse.json()[\"results\"][0][\"text\"]\n",
    "            responses.append(response)\n",
    "        total_responses.append(responses)\n",
    "    \n",
    "    # save as dataframe\n",
    "    new_dic = {}\n",
    "    new_dic[\"id\"] = items.id.tolist()\n",
    "    new_dic[\"item_text\"] = items.item_text.tolist()\n",
    "    for k, values in enumerate(total_responses): # paste responses to columns\n",
    "        new_dic[f'response_{k+1}'] = values\n",
    "        \n",
    "    df_responses_raw = pd.DataFrame(new_dic)\n",
    "    df_responses_raw.to_csv(\"../results/\" + d + \"_llama2_raw.csv\", index=False) #save raw data\n",
    "\n",
    "    #clean data (only save numeric data)\n",
    "    col_responses = df_responses_raw.columns[2:]\n",
    "    df_responses_cleaned = df_responses_raw.copy()\n",
    "    df_responses_cleaned[col_responses] = df_responses_cleaned[col_responses].applymap(lambda x: re.findall(r'\\d+', x)) #clean \n",
    "    df_responses_cleaned[col_responses] = df_responses_cleaned[col_responses].applymap(lambda x: x[0] if x else np.nan)\n",
    "    df_responses_cleaned.to_csv(\"../results/\" + d + \"_llama2.csv\", index=False) #save processed data\n",
    "    df_list.append(df_responses_cleaned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
