---
title: "survey_analyses"
author: "Suhaib Abdurahman"
date: "2023-06-14"
output: html_document
---

```{r setup, include=FALSE, warning=F, error=F}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(misty)
library(data.table)
library(rstatix)
library(car)
library(Hmisc)
libraries(sjPlot)
library(DescTools)
library(ggrain)
library(jtools)
```

## Import Response Data

### Set dataset to investigate here:
```{r}
rm(list = ls()) #potentially add other demographics like social class
data = "bigfive" # change for all respective surveys
```

### Load specified data

```{r, warning=F}
evals_dir = "../results/"
survey_dir = "../data/processed/"

alt_path1 = paste(evals_dir, data, "_", "ALT1", ".csv", sep="") #chatGPT
alt_path2 = paste(evals_dir, data, "_", "ALT2", ".csv", sep="") #chatGPT
alt_path3 = paste(evals_dir, data, "_", "ALT3", ".csv", sep="") #chatGPT
original_path = paste(evals_dir, data, ".csv", sep="")
survey_path = paste(survey_dir, data, "_cleaned", ".csv", sep="") #chatGPT

df_gpt <- read.csv(original_path) # load predictions
df_gpt <- df_gpt %>% mutate(across(starts_with("response_"), ~ as.numeric(gsub("([0-9]+).*$", "\\1", .))))
order_idx = df_gpt$id

df_gpt_alt1 <- read.csv(alt_path1) %>% slice(match(order_idx, id)) # load predictions in correct order
df_gpt_alt1 <- df_gpt_alt1 %>% mutate(across(starts_with("response_"), ~ as.numeric(gsub("([0-9]+).*$", "\\1", .))))

df_gpt_alt2 <- read.csv(alt_path2) %>% slice(match(order_idx, id)) # load predictions in correct order
df_gpt_alt2 <- df_gpt_alt2 %>% mutate(across(starts_with("response_"), ~ as.numeric(gsub("([0-9]+).*$", "\\1", .))))

df_gpt_alt3 <- read.csv(alt_path3) %>% slice(match(order_idx, id)) # load predictions in correct order
df_gpt_alt3 <- df_gpt_alt3 %>% mutate(across(starts_with("response_"), ~ as.numeric(gsub("([0-9]+).*$", "\\1", .))))

df_human <- read.csv(survey_path) # load human responses
print(nrow(df_human))

# get scale endpoints for groundtruth and each alternative prompt
n_items = nrow(df_gpt)
max_val = max(df_human[1])
min_val = min(df_human[1])

max_val1 = max(df_gpt_alt1[3], na.rm = T)
min_val1 = min(df_gpt_alt1[3], na.rm = T)

max_val2 = max(df_gpt_alt2[3], na.rm = T)
min_val2 = min(df_gpt_alt2[3], na.rm = T)

max_val3 = max(df_gpt_alt3[3], na.rm = T)
min_val3 = min(df_gpt_alt3[3], na.rm = T)
```


## Constructs and reverse coding schemes
```{r}
# big5: five constructs
if(data=="bigfive"){
reverse_code = c(6,21,31,2,12,27,37,8,18,23,43,9,24,34,35,41)
constructs = list(Extraversion = c(1, 6, 11, 16, 21, 26, 31, 36), Agreeableness = c(2, 7, 12, 17, 22, 27, 32, 37, 42), Conscientiousness = c(3, 8, 13, 18, 23, 28, 33, 38, 43), Neuroticism = c(4, 9, 14, 19, 24, 29, 34, 39), Openness = c(5, 10, 15, 20, 25, 30, 35, 40, 41, 44))
}else if(data=="rwa"){
  constructs = list(RWA = 1:15)
  reverse_code = c(2,4,6,8,10,12,14)
}else if(data=="closure"){
  constructs =  list(nfcc = c(1:2,4:11,13:n_items))
  reverse_code = c()
}else if(data=="cognition"){
  constructs = list(NFC=c(1:n_items))
  reverse_code = c(3,4,5,7,8,9,12,16,17)
}else if(data=="systems_feelings"){
  constructs =  list(Systemizing = c(1:20), Emphasizing = c(21:40))
  reverse_code = c(7,9,13,14,20,22,24,25,26,27,28,29,31,35,36,37,38,39)
}else if(data=="cogref"){
  constructs =  list(Rational = 1:20, Experiential = 21:40)
  reverse_code = c(1,2,4,5,7,8,9,11,12,18,22,29,30,32,33,34,36,37,40)
}else if(data=="mfq2"){
  constructs =  list(Care = 1:6, Equality = 7:12 , Proportionality = 13:18, Loyalty = 19:24, Authority = 25:30, Purity = 31:36)
  reverse_code = c()
}
```


## Functions

```{r}
newCols <- function(df, colList, mode){
  if(mode=="gpt"){
     for(col in names(colList)){
       values = colMeans(df[colList[[col]],3:ncols], na.rm = TRUE)
        df[nrow(df) + 1,] = c(id = 0, item_text=0, values)
        df[nrow(df), 1:2] = c(col, col)
    }
  }else{
   for(col in names(colList)){
    df[col] = rowMeans(df[colList[[col]]])
  }
}
 
  return(df)
}
```

## Preprocess Data

```{r}
####### GPT
ncols = ncol(df_gpt)

# transform to numeric
df_gpt2 = df_gpt
if(length(reverse_code)!=0){
df_gpt2[reverse_code, 3:ncols] = apply(df_gpt[reverse_code, 3:ncols], 2, item.reverse, min=min_val, max=max_val)
}

# create construct scores
df_gpt2 = newCols(df_gpt2, constructs, "gpt")

# get average responses across all repeats
gpt_avg <- rowMeans(df_gpt2[, 3:ncols], na.rm = TRUE)
print(sum(is.na(df_gpt)))

####### GPT Added Context
# transform to numeric
df_gpt_alt12 = df_gpt_alt1
if(length(reverse_code)!=0){
df_gpt_alt12[reverse_code, 3:ncols] = apply(df_gpt_alt1[reverse_code, 3:ncols], 2, item.reverse, min=min_val1, max=max_val1)
}

# create construct scores
df_gpt_alt12 = newCols(df_gpt_alt12, constructs, "gpt")

# get average responses across all repeats
gpt_alt_avg1 <- rowMeans(df_gpt_alt12[, 3:ncols], na.rm = TRUE)
print(sum(is.na(df_gpt_alt1)))

####### GPT Response Rescaling
# transform to numeric
df_gpt_alt22 = df_gpt_alt2
if(length(reverse_code)!=0){
df_gpt_alt22[reverse_code, 3:ncols] = apply(df_gpt_alt2[reverse_code, 3:ncols], 2, item.reverse, min=min_val2, max=max_val2)
}

# create construct scores
df_gpt_alt22 = newCols(df_gpt_alt22, constructs, "gpt")

# get average responses across all repeats
gpt_alt_avg2 <- rowMeans(df_gpt_alt22[, 3:ncols], na.rm = TRUE)
print(sum(is.na(df_gpt_alt2)))

####### GPT Semantic Changes
# transform to numeric
df_gpt_alt32 = df_gpt_alt3
if(length(reverse_code)!=0){
df_gpt_alt32[reverse_code, 3:ncols] = apply(df_gpt_alt3[reverse_code, 3:ncols], 2, item.reverse, min=min_val3, max=max_val3)
}

# create construct scores
df_gpt_alt32 = newCols(df_gpt_alt32, constructs, "gpt")

# get average responses across all repeats
gpt_alt_avg3 <- rowMeans(df_gpt_alt32[, 3:ncols], na.rm = TRUE)
print(sum(is.na(df_gpt_alt3)))
```

### Compare Average and Variance

```{r}
# ORIGINAL
gpt_cnstr = transpose(df_gpt2[(length(gpt_avg)-length(constructs)+1):length(gpt_avg),3:ncols])
colnames(gpt_cnstr) <- names(constructs)
# Added Context
gpt_cnstr1 = transpose(df_gpt_alt12[(length(gpt_alt_avg1)-length(constructs)+1):length(gpt_alt_avg1),3:ncols])
colnames(gpt_cnstr1) <- names(constructs)
# Added Context
gpt_cnstr2 = transpose(df_gpt_alt22[(length(gpt_alt_avg2)-length(constructs)+1):length(gpt_alt_avg2),3:ncols])
colnames(gpt_cnstr2) <- names(constructs)
# Added Context
gpt_cnstr3 = transpose(df_gpt_alt32[(length(gpt_alt_avg3)-length(constructs)+1):length(gpt_alt_avg3),3:ncols])
colnames(gpt_cnstr3) <- names(constructs)

## automatically split groups, leave 1 as "other"
### Vars: sex, age, race, religion, political_opinion

varname = "political_opinion"
gpt_cnstr["group"] = "Original Prompt"
gpt_cnstr1["group"] = "Added Context"
gpt_cnstr2["group"] = "Semantic Changes"
gpt_cnstr3 = gpt_cnstr3 - max_val3  + max_val #adjust differences in scale values
gpt_cnstr3["group"] = "Response Rescaling"

df_desc = rbind(gpt_cnstr, gpt_cnstr1, gpt_cnstr2, gpt_cnstr3)
df_desc$group <- factor(df_desc$group, levels = c('Original Prompt', "Added Context", "Semantic Changes", "Response Rescaling"))

if(length(constructs)>1){
  gpt_cnstr_avg = colMeans(gpt_cnstr[, !(names(gpt_cnstr) %in% c("group"))], na.rm = TRUE)
  gpt_cnstr_sd = sapply(gpt_cnstr[, !(names(gpt_cnstr) %in% c("group"))], sd, na.rm = TRUE)
  gpt_cnstr_avg1 = colMeans(gpt_cnstr1[, !(names(gpt_cnstr1) %in% c("group"))], na.rm = TRUE)
  gpt_cnstr_sd1 = sapply(gpt_cnstr1[, !(names(gpt_cnstr1) %in% c("group"))], sd, na.rm = TRUE)
  gpt_cnstr_avg2 = colMeans(gpt_cnstr2[, !(names(gpt_cnstr2) %in% c("group"))], na.rm = TRUE)
  gpt_cnstr_sd2 = sapply(gpt_cnstr2[, !(names(gpt_cnstr2) %in% c("group"))], sd, na.rm = TRUE)
  gpt_cnstr_avg3 = colMeans(gpt_cnstr3[, !(names(gpt_cnstr3) %in% c("group"))], na.rm = TRUE)
  gpt_cnstr_sd3 = sapply(gpt_cnstr3[, !(names(gpt_cnstr3) %in% c("group"))], sd, na.rm = TRUE)
}else{
  gpt_cnstr_avg = mean(gpt_cnstr[, !(names(gpt_cnstr) %in% c("group"))], na.rm = TRUE)
  gpt_cnstr_sd = sd(gpt_cnstr[, !(names(gpt_cnstr) %in% c("group"))], na.rm = TRUE)
  gpt_cnstr_avg1 = mean(gpt_cnstr1[, !(names(gpt_cnstr1) %in% c("group"))], na.rm = TRUE)
  gpt_cnstr_sd1 = sd(gpt_cnstr1[, !(names(gpt_cnstr1) %in% c("group"))], na.rm = TRUE)
  gpt_cnstr_avg2 = mean(gpt_cnstr2[, !(names(gpt_cnstr2) %in% c("group"))], na.rm = TRUE)
  gpt_cnstr_sd2 = sd(gpt_cnstr2[, !(names(gpt_cnstr2) %in% c("group"))], na.rm = TRUE)
  gpt_cnstr_avg3 = mean(gpt_cnstr3[, !(names(gpt_cnstr3) %in% c("group"))], na.rm = TRUE)
  gpt_cnstr_sd3 = sd(gpt_cnstr3[, !(names(gpt_cnstr3) %in% c("group"))], na.rm = TRUE)
}

## Test group differences
# test differences of means on all constructs
# shows effect of variable on probability for human => positive means humans have higher mean
diff_avg = data.frame(t(gpt_cnstr_avg), row.names = c("Original Prompt"))
diff_avg["Added Context",] = diff_avg["Original Prompt",] - gpt_cnstr_avg1
diff_avg["Response Rescaling",] = diff_avg["Original Prompt",] - gpt_cnstr_avg2
diff_avg["Semantic Changes",] = diff_avg["Original Prompt",] - gpt_cnstr_avg3

diff_sd = data.frame(t(gpt_cnstr_sd), row.names = c("Original Prompt"))
diff_sd["Added Context",] = diff_sd["Original Prompt",] - gpt_cnstr_sd1
diff_sd["Response Rescaling",] = diff_sd["Original Prompt",] - gpt_cnstr_sd2
diff_sd["Semantic Changes",] = diff_sd["Original Prompt",] - gpt_cnstr_sd3

#Dunnet, are group means different from chatGPT?
for(col in colnames(df_desc)){
  if(col != "group"){
    f = as.formula(paste(col, "~ group", collapse = ""))
    model <- aov(f, data = df_desc)
    print(col)
    print(DunnettTest(f, data = df_desc, control = "Original Prompt"))
  }
}

print(diff_avg)
print("Bonferroni corrected p-value:")
print(0.05/length(constructs))
  
print("Variance Test")
# test differences of variance on all constructs
for(g in unique(df_desc$group)){
  if(g!="Original Prompt"){
    df1 = df_desc %>% filter(group==g | group=="Original Prompt") %>% select(names(constructs))
    df2 = df_desc %>% filter(group==g | group=="Original Prompt")
    print(g)
    print(apply(df1,2,function(x) 
  {leveneTest(x ~ as.factor(df2$group))[1,3]}))
  }
}

print(diff_sd)
print("Bonferroni corrected p-value:")
print(0.05/length(unique(df_desc$group))/length(constructs))

desc_cols = colnames(df_desc)
desc_cols = desc_cols[1:length(desc_cols)-1]
df_comparison <- gather(df_desc, construct, value, all_of(desc_cols), factor_key=TRUE)

t1 <- ggplot(df_comparison, aes(x = construct, y = value, fill = group)) +
      PupillometryR::geom_flat_violin(aes(fill = group), scale = "width", position = position_nudge(x = .2, y = 0), adjust = 1.5, trim = FALSE, alpha = .35, colour = NA)+
      geom_point(aes(x = construct, y = value, colour = group), position = ggpp::position_jitternudge(width=0.05, height=0.075, x = -.25, nudge.from = "jittered"), size = 1, shape = 20, alpha = 0.5)+
      geom_boxplot(aes(x = construct, y = value, fill = group),outlier.shape = NA, lwd=.1, alpha = .5, width = .35, colour = "black")+
      scale_colour_brewer(palette = "Set1")+ scale_fill_brewer(palette = "Set1")+ xlab("") + ylab("Construct Score") + theme_apa(y.font.size=11)
    
    print(t1)
    
    ggsave(filename = paste("../results/plots/", data, "_prompting.pdf", sep=""), t1, width = 8, height = 4, dpi = 150, units = "in", device='pdf')
```


