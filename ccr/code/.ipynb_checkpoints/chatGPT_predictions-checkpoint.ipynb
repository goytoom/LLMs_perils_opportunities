{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ab5420-7666-483b-b79e-e871582d7f74",
   "metadata": {},
   "source": [
    "ADD EXPLANATION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c610073c-59b6-418c-b165-a5bc7e96d001",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca7c26cf-9783-4ba8-b0c2-79a8a2d5c4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re\n",
    "remove = string.punctuation\n",
    "remove = remove.replace(\"-\", \"\").replace(\",\", \"\") # don't remove hyphens\n",
    "pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import logging\n",
    "from retry import retry\n",
    "logging.basicConfig()\n",
    "\n",
    "# Calculate the delay based on your rate limit\n",
    "rate_limit_per_minute = 3500.0\n",
    "delay_60 = 60.0 / 60\n",
    "delay_full = 60.0 / rate_limit_per_minute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d4601-a181-4fc0-9ff0-efb3f0d8b0ff",
   "metadata": {},
   "source": [
    "## General Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40c53c4-f26c-4dc7-babe-39cb77569822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode = \"full\"\n",
    "folder = \"../data/\"\n",
    "path_texts = folder + \"CCR_clean.csv\"\n",
    "path_items = folder + \"CCR_items.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99e6b8-baea-42dc-8467-dafcd97472b8",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8410c23-d642-40ea-bb96-b6392c4aeec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chatGPT parameters\n",
    "openai.api_key = \"\" #add your openai key here!\n",
    "model_engine = \"gpt-3.5-turbo-0301\"\n",
    "\n",
    "@retry(delay=5)\n",
    "def delayed_completion(delay_in_seconds: float = 1, **kwargs):\n",
    "    \"\"\"Delay a completion by a specified amount of time.\"\"\"\n",
    "\n",
    "    # Sleep for the delay\n",
    "    time.sleep(delay_in_seconds)\n",
    "\n",
    "    # Call the Completion API and return the result\n",
    "    return openai.ChatCompletion.create(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75469f4-5b5f-4463-bef5-1f325d533c7d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14bb2c2c-b086-4341-90be-edcab5fad420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_texts = pd.read_csv(path_texts)\n",
    "df_items = pd.read_csv(path_items)\n",
    "text_values = df_texts.ValuesSurvey.values.tolist()\n",
    "text_behaviors = df_texts.BehaviorsSurvey.values.tolist()\n",
    "\n",
    "#rename constructs to something readable\n",
    "# can be left out if all construct names are correct in the raw data/\n",
    "constr_rename_dict = {\"CARE_tot\": \"Care values\", \"EQUALITY_tot\": \"Equality values\", \"PROPORTIONALITY_tot\": \"Proportionality values\", \"LOYALTY_tot\": \"Loyalty values\", \n",
    "               \"AUTHORITY_tot\": \"Authority values\", \"PURITY_tot\": \"Purity values\",\n",
    "               \"Individualism_tot\": \"Individualism\", \"Collectivism_tot\": \"Collectivism\", \"Religiosity_tot\": \"Religiosity\", \n",
    "               \"conservatism_tot\": \"Conservatism\", \"NFC_tot\": \"Need for Cognition\", \"Tightness\": \"Cultural Tightness\", \"PVQ_SD_tot\": \"Self-direction values\", \n",
    "               \"PVQ_PO_tot\": \"Power values\", \"PVQ_UN_tot\": \"Universalism\", \"PVQ_AC_tot\": \"Achievement values\", \"PVQ_SE_tot\": \"Security values\", \"PVQ_ST_tot\": \"Stimulation\", \"PVQ_CO_tot\": \"Conformity\",\n",
    "               \"PVQ_TR_tot\": \"Tradition\", \"PVQ_HE_tot\": \"Hedonism\", \"PVQ_BE_tot\": \"Benevolence values\",\n",
    "} \n",
    "\n",
    "#alternatively read the construct/survey list directly from a file\n",
    "constructs = constr_rename_dict.keys()\n",
    "\n",
    "# define scale endpoints for dynamic addition in prompt\n",
    "scale_meaning_dict = {}\n",
    "for key in constr_rename_dict.keys():\n",
    "    if key==\"Religiosity_tot\":\n",
    "        scale_meaning_dict[key] = [\"never or definitely not true of me\", \"very frequently or definitely true of me\", 1, 6] #SCALE end points in word and number\n",
    "    elif key==\"conservatism_tot\":\n",
    "        scale_meaning_dict[key] = [\"completely disagree\", \"completely agree\", 1, 7]\n",
    "    elif \"PVQ_\" in key:\n",
    "        scale_meaning_dict[key] = [\"not like me at all\", \"very much like me\", 1, 6]\n",
    "    elif \"NFC\" in key:\n",
    "        scale_meaning_dict[key] = [\"extremely uncharacteristic of me\", \"extremely characteristic of me\", 1, 5]\n",
    "    else:\n",
    "        scale_meaning_dict[key] = [\"strongly disagree\", \"strongly agree\", 1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19aad1f1-e986-49fd-b284-8ceca5392a49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def createPrompts(texts, construct, mode=1):\n",
    "    items = df_items[construct].dropna().values.tolist()\n",
    "    min_val = df_texts[construct].min()\n",
    "    max_val = df_texts[construct].max()\n",
    "    constr_name = constr_rename_dict[construct]\n",
    "    meaning_min, meaning_max, min_val, max_val = scale_meaning_dict[construct]\n",
    "    if mode==1: #general\n",
    "        prompts = [\"For a scientific study, rate how strongly the author of the following text endorses \\\"{}\\\" solely based on the text they have written. \"\\\n",
    "        \"Here is the text: \\n\"\\\n",
    "        \"\\\"{}\\\"\\n\\n\" \\\n",
    "        \"Respond with a single number of up to two decimal points and between {} and {}, with {} meaning \\\"{}\\\" and {} meaning \\\"{}\\\". \"\\\n",
    "        \"Respond only with this single number and nothing else. Do not use words.\".format(constr_name,text, min_val, max_val, min_val, meaning_min, max_val, meaning_max) for text in texts]\n",
    "    elif mode==2: #hybrid: chatGPT + CCR idea (rating on items)\n",
    "        item_string = \"\".join([str(i+1) + \") \\\"\" + str(item) + \"\\\"\\n\" for i,item in enumerate(items)])\n",
    "        prompts = [\"For a scientific study, rate how strongly the author of the following text endorses the following psychological items solely based on the text they have written. \"\\\n",
    "        \"Here is the text: \\n\"\\\n",
    "        \"\\\"{}\\\"\\n\\n\" \\\n",
    "        \"Here are the items: \\n\"\\\n",
    "        \"{}\\n\"\\\n",
    "        \"Respond to each item with a single digit between {} and {}, with {} meaning \\\"{}\\\" and {} meaning \\\"{}\\\". \"\\\n",
    "        \"Respond with exactly {} numbers, comma separated. Do not use words.\".format(text, item_string, min_val, max_val, min_val, meaning_min, max_val, meaning_max, str(len(items))) for text in texts]\n",
    "    else:\n",
    "        pass\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6122c06e-d001-4e29-8564-e18f4453bdfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e99d9776-c62e-448d-a7a7-f4cab94b15dd",
   "metadata": {},
   "source": [
    "## Test Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "541dd2a5-4152-4d92-b749-2e56cc813b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'For a scientific study, rate how strongly the author of the following text endorses \"Universalism\" solely based on the text they have written. Here is the text: \\n\"I’m always lost. I’m glad I met my husband. He not only supports me on everything I do but guides me to the right path. I would tell him everything or whatever that bothers and he’ll try to talk me through and help me. So whenever I make a decision, I’ll talk to him about it.\"\\n\\nRespond with a single number of up to two decimal points and between 1 and 6, with 1 meaning \"not like me at all\" and 6 meaning \"very much like me\". Respond only with this single number and nothing else. Do not use words.'}\n",
      "\n",
      "{'role': 'user', 'content': 'For a scientific study, rate how strongly the author of the following text endorses the following psychological items solely based on the text they have written. Here is the text: \\n\"I’m always lost. I’m glad I met my husband. He not only supports me on everything I do but guides me to the right path. I would tell him everything or whatever that bothers and he’ll try to talk me through and help me. So whenever I make a decision, I’ll talk to him about it.\"\\n\\nHere are the items: \\n1) \"I believe that every person in the world should be treated equally.\"\\n2) \"I believe in listening to people who are different from me and try to understand them.\"\\n3) \"I strongly believe that we should care about nature.\"\\n\\nRespond to each item with a single digit between 1 and 6, with 1 meaning \"not like me at all\" and 6 meaning \"very much like me\". Respond with exactly 3 numbers, comma separated. Do not use words.'}\n"
     ]
    }
   ],
   "source": [
    "# Choose a construct and check prompts\n",
    "constr = \"PVQ_UN_tot\"\n",
    "for texts in [text_values, text_behaviors]:\n",
    "    prompts_general = createPrompts(text_values, constr, 1)\n",
    "    prompts_hybrid = createPrompts(text_values, constr, 2)\n",
    "    messages_general = [{\"role\": \"user\", \"content\": x} for x in prompts_general]\n",
    "    messages_hybrid = [{\"role\": \"user\", \"content\": x} for x in prompts_hybrid]\n",
    "\n",
    "print(messages_general[42])\n",
    "print()\n",
    "print(messages_hybrid[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418630d3-d0cf-4d36-b0be-21ee65848b5a",
   "metadata": {},
   "source": [
    "### Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52250c34-9345-4845-9042-8c772c56e102",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00\n"
     ]
    }
   ],
   "source": [
    "# construct-level\n",
    "APIresponse = delayed_completion(\n",
    "    delay_in_seconds=delay_full,\n",
    "    model=model_engine,\n",
    "    messages=[messages_general[42]],\n",
    "    temperature=0\n",
    "    )\n",
    "response = APIresponse.choices[0].message[\"content\"]\n",
    "print(response) #works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05dbd289-85b7-4355-8597-665a651a29e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3\n"
     ]
    }
   ],
   "source": [
    "# item-level\n",
    "APIresponse = delayed_completion(\n",
    "    delay_in_seconds=delay_full,\n",
    "    model=model_engine,\n",
    "    messages=[messages_hybrid[42]],\n",
    "    temperature=0\n",
    "    )\n",
    "response = APIresponse.choices[0].message[\"content\"]\n",
    "print(response) #works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce67b9f-4dbc-4ffb-9e66-bbf82f6881f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c57ca5-e53a-4bef-bce8-db79611d7536",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for constr in constructs:\n",
    "    print(constr)\n",
    "    for source,texts in zip([\"values\", \"behaviors\"], [text_values, text_behaviors]):\n",
    "        prompts_general = createPrompts(texts, constr, 1)\n",
    "        prompts_hybrid = createPrompts(texts, constr, 2)\n",
    "        messages_general = [{\"role\": \"user\", \"content\": x} for x in prompts_general]\n",
    "        messages_hybrid = [{\"role\": \"user\", \"content\": x} for x in prompts_hybrid]\n",
    "        print(source)\n",
    "        for i, message in enumerate(messages_general):\n",
    "            row_name = \"Participant_\" + str(i+1)\n",
    "            # print(row_name)\n",
    "            APIresponse = delayed_completion(\n",
    "                delay_in_seconds=delay_full,\n",
    "                model=model_engine,\n",
    "                messages=[message],\n",
    "                temperature=0,\n",
    "                )\n",
    "            response = APIresponse.choices[0].message[\"content\"]\n",
    "            rows.append([row_name, source, \"general\", constr, response])\n",
    "\n",
    "        for i, message in enumerate(messages_hybrid):\n",
    "            row_name = \"Participant_\" + str(i+1)\n",
    "            APIresponse = delayed_completion(\n",
    "                delay_in_seconds=delay_full,\n",
    "                model=model_engine,\n",
    "                messages=[message],\n",
    "                temperature=0,\n",
    "                )\n",
    "            response = APIresponse.choices[0].message[\"content\"]\n",
    "            rows.append([row_name, source, \"hybrid\", constr, response])\n",
    "            \n",
    "# save as dataframe\n",
    "df_predictions = pd.DataFrame(rows, columns=[\"id\", \"source\", \"prompt_type\", \"construct\", \"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34d223cb-f503-4c94-8979-453a74e5f512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#format the ChatGPT results into final shape (from wide to long format)\n",
    "df_ratings = df_texts.copy()\n",
    "df_ratings.insert(0, \"id\", [\"Participant_\" + str(i+1) for i in range(len(messages_general))])\n",
    "df_ratings = pd.melt(df_ratings, id_vars=[x for x in df_ratings.columns if x not in constr_rename_dict.keys()], value_vars=constr_rename_dict.keys(), value_name=\"response\", var_name=\"construct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a80d67e1-0d06-4c9b-bb1b-caee9f5087f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\chatgpt\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    }
   ],
   "source": [
    "# merge datasets and extract numbers (clean the responses so that the predictions are only numbers)\n",
    "df_final = df_ratings.merge(df_predictions, \"left\", on=[\"id\", \"construct\"]).dropna(subset=[\"prediction\"], axis=0) #save only what was predicted\n",
    "df_final[\"prediction_clean\"] = df_final.prediction.apply(lambda x: re.sub(r\",\\s*\\d+\", \"\", x) if \")\" in x else x) #choose first number if gpt reports more than one for an item\n",
    "df_final[\"prediction_clean\"] = df_final.prediction_clean.apply(lambda x: re.sub(r\"\\d+\\)\", \"\", x) if x else x) #remove parenthesis and numbering in case gpt uses it for item-level responses\n",
    "df_final[\"prediction_clean\"] = df_final.prediction_clean.apply(lambda x: [float(i) for i in re.findall(r'(\\d+(?:\\.\\d+)?)', x)])\n",
    "df_final[\"prediction_clean_avg\"] = df_final.prediction_clean.apply(lambda x: np.mean(x))\n",
    "\n",
    "# split multi-item predictions into individual items\n",
    "df_final = df_final.explode('prediction_clean').reset_index(drop=True)\n",
    "df_final['item'] = df_final.groupby(['id', 'construct', \"source\", \"prompt_type\"]).transform('cumcount').add(1)\n",
    "df_final['item'] = [\"item_\" + str(x) for x in df_final.item]\n",
    "df_final.loc[df_final.prompt_type==\"general\", 'item'] = \"total\" #rename construct level predictions\n",
    "\n",
    "# save\n",
    "df_final.to_csv(\"../results/\" + choice_name + \".csv\", index=False)\n",
    "\n",
    "# print number of NAs (out of N constructs x M participants x 2 essays)\n",
    "print(df_final.shape[0] - df_final.dropna().shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
