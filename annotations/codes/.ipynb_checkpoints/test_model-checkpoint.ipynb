{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ccec441-464b-4b1e-abf6-72d9c58083af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import load_model \n",
    "from keras.metrics import Precision, Recall\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import tokenization\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "foundations = {\"mftc\": {\n",
    "                    \"binding\": [\"individual\", \"binding\"], \n",
    "                    \"moral\": [\"moral\"],\n",
    "                    \"full\": [\"care\", \"fairness\", \"loyalty\", \"authority\", \"purity\"],\n",
    "                    \"complete\": [\"care\", \"harm\", \"fairness\", \"cheating\", \"loyalty\", \"betrayal\", \"authority\", \"subversion\", \"purity\", \"degradation\"]\n",
    "                },\n",
    "               \"mfrc\":  {\n",
    "                    \"binding\": [\"individual\", \"binding\", \"proportionality\", \"thin morality\"], \n",
    "                    \"moral\": [\"moral\", \"thin morality\"],\n",
    "                    \"full\": [\"care\", \"proportionality\", \"loyalty\", \"authority\", \"purity\", \"equality\", \"thin morality\"],\n",
    "                    \"complete\": [\"care\", \"harm\", \"equality\", \"proportionality\", \"loyalty\", \"betrayal\", \"authority\", \"subversion\", \"purity\", \"degradation\", \"thin morality\"]\n",
    "               }\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c002d5d-2d54-4ab6-98a6-2abdd6620dc1",
   "metadata": {},
   "source": [
    "Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "628b61f4-47da-4c29-866d-0a2ccec0fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512, classes = 5, activation = \"sigmoid\"):\n",
    "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    outputs= bert_layer(dict(input_word_ids=input_word_ids,\n",
    "    input_mask=input_mask,\n",
    "    input_type_ids=segment_ids))\n",
    "\n",
    "    # pooled_output=outputs[\"pooled_output\"]\n",
    "    sequence_output=outputs[\"sequence_output\"]\n",
    "\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    # net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
    "    # net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    out = tf.keras.layers.Dense(classes, activation=activation)(net)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                  loss='binary_crossentropy', metrics=[Precision(), Recall()])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_binary(_y, threshold):\n",
    "    y = _y.copy()\n",
    "    y[y >= threshold] = 1\n",
    "    y[y < threshold] = 0\n",
    "    return y\n",
    "\n",
    "def F1Measure(y_true, y_pred, threshold=0.5):\n",
    "    y_binary = get_binary(y_pred, threshold)\n",
    "    score = f1_Score(y_true, y_binary, average = \"macro\")   \n",
    "\n",
    "    return score\n",
    "\n",
    "def train(mode, bert_layer, corp):\n",
    "    \n",
    "    classes = {\"mfrc\": {\"full\": 8, \"moral\": 3, \"binding\": 5, \"complete\": 12}, \"mftc\": {\"full\": 6, \"moral\": 2, \"binding\": 3, \"complete\": 11}}\n",
    "    activation = {\"full\": \"sigmoid\", \"moral\": \"softmax\", \"binding\": \"softmax\"}\n",
    "    model = build_model(bert_layer, max_len=256, classes = classes[corp][mode], activation = activation[mode])\n",
    "\n",
    "    with open(\"../data/train_test/\" + corp + \"_train_\" + mode + \".pkl\", \"rb\") as f:\n",
    "        X_train, y_train = pkl.load(f)\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint('../models/' + corp + '_normalmodel_' + mode + '.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "    print(\"start training\")\n",
    "    t = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.1,\n",
    "        epochs=200,\n",
    "        callbacks=[checkpoint, earlystopping],\n",
    "        batch_size=32, #32 works best so far\n",
    "        verbose=1)\n",
    "    # print(t)\n",
    "    print(\"Saving the model\")\n",
    "    # t.save\n",
    "\n",
    "def crossVal(mode, bert_layer, corp):\n",
    "    \n",
    "    classes = {\"mfrc\": {\"full\": 8, \"moral\": 3, \"binding\": 5, \"complete\": 12}, \"mftc\": {\"full\": 6, \"moral\": 2, \"binding\": 3, \"complete\": 11}}\n",
    "    activation = {\"full\": \"sigmoid\", \"moral\": \"sigmoid\", \"binding\": \"sigmoid\"}\n",
    "    model = build_model(bert_layer, max_len=256, classes = classes[corp][mode], activation = activation[mode])\n",
    "\n",
    "    # print(model.summary())\n",
    "    with open(\"../data/train_test/\" + corp + \"_train_\" + mode + \".pkl\", \"rb\") as f:\n",
    "        X, y = pkl.load(f)\n",
    "\n",
    "    # checkpoint = tf.keras.callbacks.ModelCheckpoint('../models/' + corp + '_model_' + mode + '.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "    print(\"Start Cross-Validation\")\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    cvscores = []\n",
    "    for train, test in kfold.split(X[0], reverse_onehot(y)): #potentially use CV folds as predictions to evaluate against chatGPT\n",
    "        X_train_cv = (X[0][train], X[1][train], X[2][train])\n",
    "        y_train_cv = tf.gather(y_train, train)\n",
    "        X_test_cv = (X[0][test], X[1][test], X[2][test])\n",
    "        y_test_cv = tf.gather(y, test)\n",
    "        t = model.fit(\n",
    "            X_train_cv, y_train_cv,\n",
    "            validation_data = (X_test_cv, y_test_cv),\n",
    "            # validation_split=0.2,\n",
    "            epochs=200,\n",
    "            callbacks=[earlystopping], #[checkpoint, earlystopping]\n",
    "            batch_size=32, #32 works best so far\n",
    "            verbose=0)\n",
    "        \n",
    "        y_pred_val = model.predict(X_test_cv, y_test_cv)\n",
    "        scores = F1Measure(y_test_cv, y_pred_val, 0.5)\n",
    "        print(\"%s: %.2f%%\" % (\"f1_score\", scores[1]*100))\n",
    "        cvscores.append(scores[1] * 100)\n",
    "        \n",
    "        # print(t)\n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "    # print(\"Saving the model\")\n",
    "    # t.save\n",
    "\n",
    "def full_train(mode, bert_layer, corp):\n",
    "    \n",
    "    classes = {\"mfrc\": {\"full\": 8, \"moral\": 3, \"binding\": 5, \"complete\": 13}, \"mftc\": {\"full\": 6, \"moral\": 3, \"binding\": 3, \"complete\": 11}}\n",
    "    activation = {\"full\": \"sigmoid\", \"moral\": \"sigmoid\", \"binding\": \"sigmoid\"}\n",
    "    model = build_model(bert_layer, max_len=256, classes = classes[corp][mode], activation = activation[mode])\n",
    "\n",
    "    with open(\"../data/train_test/\" + corp + \"_fulltrain_\" + mode + \".pkl\", \"rb\") as f:\n",
    "        X, y = pkl.load(f)\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint('../models/' + corp + '_crossmodel_' + mode + '.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "    print(\"start training\")\n",
    "    t = model.fit(\n",
    "        X, y,\n",
    "        validation_split = 0.2,\n",
    "        epochs=200,\n",
    "        callbacks=[checkpoint, earlystopping],\n",
    "        batch_size=32, #32 works best so far\n",
    "        verbose=1)\n",
    "    # print(t)\n",
    "    print(\"Saving the model\")\n",
    "    # t.save\n",
    "\n",
    "def reverse_onehot(onehot_data):\n",
    "    # onehot_data assumed to be channel last\n",
    "    data_copy = np.zeros(onehot_data.shape[:-1])\n",
    "    for c in range(onehot_data.shape[-1]):\n",
    "        img_c = onehot_data[..., c]\n",
    "        data_copy[img_c == 1] = c\n",
    "    return data_copy\n",
    "    \n",
    "# def evaluate(model_file, data_file, bert_layer, threshold=0.5):\n",
    "\n",
    "#     model = load_model(model_file, compile=True, custom_objects={\"KerasLayer\": bert_layer})\n",
    "\n",
    "#     with open(data_file, \"rb\") as f:\n",
    "#         X_test, y_test = pkl.load(f)\n",
    "\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     print('predicted')\n",
    "    \n",
    "#     f1_score = F1Measure(y_test, y_pred, threshold=0.9)\n",
    "#     print(f\"threshold: {0.9}, score :{f1_score}\")\n",
    "#     f1_score = F1Measure(y_test, y_pred, threshold=0.8)\n",
    "#     print(f\"threshold: {0.8}, score :{f1_score}\")\n",
    "#     f1_score = F1Measure(y_test, y_pred, threshold=0.7)\n",
    "#     print(f\"threshold: {0.7}, score :{f1_score}\")\n",
    "#     f1_score = F1Measure(y_test, y_pred, threshold=0.6)\n",
    "#     print(f\"threshold: {0.6}, score :{f1_score}\")\n",
    "#     f1_score = F1Measure(y_test, y_pred, threshold=0.5)\n",
    "#     print(f\"threshold: {0.5}, score :{f1_score}\")\n",
    "#     f1_score = F1Measure(y_test, y_pred, threshold=0.4)\n",
    "#     print(f\"threshold: {0.4}, score :{f1_score}\")\n",
    "#     f1_score = F1Measure(y_test, y_pred, threshold=0.3)\n",
    "#     print(f\"threshold: {0.3}, score :{f1_score}\")\n",
    "#     f1_score = F1Measure(y_test, y_pred, threshold=0.2)\n",
    "#     print(f\"threshold: {0.2}, score :{f1_score}\")\n",
    "#     f1_score = F1Measure(y_test, y_pred, threshold=0.1)\n",
    "#     print(f\"threshold: {0.1}, score :{f1_score}\")\n",
    "\n",
    "#     return 0\n",
    "    \n",
    "module_url = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/2\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d43a9276-36a2-4efb-b307-4adbc41b62d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # corp = \"mfrc\"\n",
    "# corp = sys.argv[1]\n",
    "\n",
    "# # mode = \"full\n",
    "# mode = sys.argv[2]\n",
    "# training = sys.argv[3]\n",
    "\n",
    "corp = \"mftc\"\n",
    "mode = \"full\"\n",
    "training = \"eval\"\n",
    "\n",
    "# data_file = \"../data/train_test/\" + corp + \"_test_\" + mode + \".pkl\"\n",
    "# model_file = \"../models/\" + corp + \"_model_\" + mode + \".h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dea8c691-cce9-4e34-820a-a011e9e109d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_mask (InputLayer)        [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " input_word_ids (InputLayer)    [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " keras_layer_7 (KerasLayer)     {'pooled_output': (  17488641    ['input_mask[0][0]',             \n",
      "                                None, 256),                       'segment_ids[0][0]',            \n",
      "                                 'sequence_output':               'input_word_ids[0][0]']         \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 256, 256),                                               \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256)],                                               \n",
      "                                 'default': (None,                                                \n",
      "                                256)}                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_12 (S  (None, 256)         0           ['keras_layer_7[1][14]']         \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 64)           16448       ['tf.__operators__.getitem_12[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 64)           0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 6)            390         ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,505,479\n",
      "Trainable params: 17,505,478\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "Start Cross-Validation\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;66;03m# determine best model using CV\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mcrossVal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m training \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;66;03m# train on full corpus for cross corpus predictions\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     full_train(mode, bert_layer, corp)\n",
      "Cell \u001b[1;32mIn[130], line 83\u001b[0m, in \u001b[0;36mcrossVal\u001b[1;34m(mode, bert_layer, corp)\u001b[0m\n\u001b[0;32m     81\u001b[0m X_test_cv \u001b[38;5;241m=\u001b[39m (X_train[\u001b[38;5;241m0\u001b[39m][test], X_train[\u001b[38;5;241m0\u001b[39m][test], X_train[\u001b[38;5;241m0\u001b[39m][test])\n\u001b[0;32m     82\u001b[0m y_test_cv \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(y_train,test)\n\u001b[1;32m---> 83\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_cv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_cv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_cv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_cv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# validation_split=0.2,\u001b[39;49;00m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearlystopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#[checkpoint, earlystopping]\u001b[39;49;00m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#32 works best so far\u001b[39;49;00m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m y_val \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_cv, y_test_cv)\n\u001b[0;32m     93\u001b[0m scores \u001b[38;5;241m=\u001b[39m F1Measure(y_test_cv, y_val, \u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[1;32mD:\\Programmes\\Anaconda\\envs\\mftc\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\Programmes\\Anaconda\\envs\\mftc\\Lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mD:\\Programmes\\Anaconda\\envs\\mftc\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\Programmes\\Anaconda\\envs\\mftc\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mD:\\Programmes\\Anaconda\\envs\\mftc\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mD:\\Programmes\\Anaconda\\envs\\mftc\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Programmes\\Anaconda\\envs\\mftc\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mD:\\Programmes\\Anaconda\\envs\\mftc\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mD:\\Programmes\\Anaconda\\envs\\mftc\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if training == \"eval\": # determine best model using CV\n",
    "    crossVal(mode, bert_layer, corp)\n",
    "elif training == \"cross\": # train on full corpus for cross corpus predictions\n",
    "    full_train(mode, bert_layer, corp)\n",
    "elif training == \"normal\": # regular training for test sample (against chatGPT)\n",
    "    train(mode, bert_layer, corp)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9437db85-84f8-4371-83c6-7e485adf48a3",
   "metadata": {},
   "source": [
    "Functions for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a725ea7b-7b2e-42fd-95eb-e534a55241b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import load_model \n",
    "import tokenization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb0f273-7dc0-4173-a0fe-38bcbbbf2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)    \n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4fb6b31-cb7a-4e16-87f2-16f6d58611c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "foundations = {\"mftc\": {\n",
    "                    \"binding\": [\"individual\", \"binding\", \"non-moral\"], \n",
    "                    \"moral\": [\"moral\", \"non-moral\"],\n",
    "                    \"full\": [\"care\", \"fairness\", \"loyalty\", \"authority\", \"purity\", \"non-moral\"],\n",
    "                    \"complete\": [\"care\", \"harm\", \"fairness\", \"cheating\", \"loyalty\", \"betrayal\", \"authority\", \"subversion\", \"purity\", \"degradation\", \"non-moral\"]\n",
    "                },\n",
    "               \"mfrc\":  {\n",
    "                    \"binding\": [\"individual\", \"binding\", \"proportionality\", \"thin morality\", \"non-moral\"], \n",
    "                    \"moral\": [\"moral\", \"thin morality\", \"non-moral\"],\n",
    "                    \"full\": [\"care\", \"proportionality\", \"loyalty\", \"authority\", \"purity\", \"equality\", \"thin morality\", \"non-moral\"],\n",
    "                    \"complete\": [\"care\", \"harm\", \"equality\", \"proportionality\", \"loyalty\", \"betrayal\", \"authority\", \"subversion\", \"purity\", \"degradation\", \"thin morality\", \"non-moral\"]\n",
    "               }\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "466bcaf3-8155-488b-a47d-a721e79cf246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Functions\n",
    "def pre_process_text(X, tokenizer):\n",
    "    tokenized = [tokenizer.tokenize(x) for x in X]\n",
    "    results = []\n",
    "    drops = []\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "\n",
    "    en_stopwords = stopwords.words('english')\n",
    "    for i, text in enumerate(tokenized):\n",
    "        out = [token for token in text if (token not in en_stopwords) and (token not in symbols)\n",
    "               and (not token.startswith(\"@\")\n",
    "                    and (not token.startswith(\"http\")))]\n",
    "        if len(out) >= 5:               # remove tweets that are too short after preprocessing\n",
    "            results.append(out)\n",
    "        else:\n",
    "            drops.append(i)\n",
    "    return results, drops\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=256):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        text = text[:max_len - 2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "\n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "\n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "\n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "def get_binary(_y, threshold):\n",
    "    y = _y.copy()\n",
    "    y[y >= threshold] = 1\n",
    "    y[y < threshold] = 0\n",
    "    return y\n",
    "\n",
    "\n",
    "def predict(df, mode, threshold):\n",
    "   \n",
    "    cols = foundations[corp][mode]\n",
    "    model_file = '../models/' + corp + '_' + type + 'model_' + mode + '.h5'\n",
    "    model = load_model(model_file, compile=True, custom_objects={\"KerasLayer\": bert_layer})\n",
    "    \n",
    "    X_raw = list(df[\"text\"])\n",
    "    X, idx_drop = pre_process_text(X_raw, tokenizer)\n",
    "    print(len(idx_drop))\n",
    "    X = bert_encode(X, tokenizer)\n",
    "    y_pred_proba = model.predict(X)\n",
    "    y_pred = get_binary(y_pred_proba, threshold)\n",
    "    df_dropped = df.drop(idx_drop, axis = 0)\n",
    "    df_dropped[cols] = pd.DataFrame(y_pred, index=df_dropped.index)\n",
    "    \n",
    "    # save predictions\n",
    "    df_dropped.to_csv(\"../results/\" + corp + \"_labels_\" + type + \"_\" + mode + \".csv\", index=False)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac4cb9c-5d54-4d20-b9d4-c656fb69da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Get parameters and run predictions\n",
    "# corp = sys.argv[1]\n",
    "# mode = sys.argv[2]\n",
    "# type = sys.argv[3]\n",
    "# threshold = float(sys.argv[4])\n",
    "\n",
    "corp = \"mftc\"\n",
    "mode = \"full\"\n",
    "type = \"normal\"\n",
    "threshold = 0.5\n",
    "\n",
    "if type == \"cross\": #for cross corpus prediction: apply trained model on the other corpus\n",
    "    if corp == \"mftc\"\n",
    "        file_path = \"../data/preprocessed/mfrc_cleaned_\" + mode + \".csv\"\n",
    "    elif corp == \"mfrc\":\n",
    "        file_path = \"../data/preprocessed/mftc_cleaned_\" + mode + \".csv\"\n",
    "elif type == \"normal:\n",
    "    file_path = \"../data/preprocessed/\" + corp + \"_sample_\" + mode + \".csv\"\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#get annoatations of texts\n",
    "file = pd.read_csv(file_path)\n",
    "predict(file, mode, threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
