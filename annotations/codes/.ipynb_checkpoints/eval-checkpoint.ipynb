{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ccc9d66-894f-4d38-ae66-2843b1053dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols, logit\n",
    "\n",
    "foundations = {\"mftc\": {\n",
    "                    \"binding\": [\"individual\", \"binding\", \"non-moral\"], \n",
    "                    \"moral\": [\"moral\", \"non-moral\"],\n",
    "                    \"full\": [\"care\", \"fairness\", \"loyalty\", \"authority\", \"purity\", \"non-moral\"],\n",
    "                    \"complete\": [\"care\", \"harm\", \"fairness\", \"cheating\", \"loyalty\", \"betrayal\", \"authority\", \"subversion\", \"purity\", \"degradation\", \"non-moral\"]\n",
    "                },\n",
    "               \"mfrc\":  {\n",
    "                    \"binding\": [\"individual\", \"binding\", \"proportionality\", \"thin morality\", \"non-moral\"], \n",
    "                    \"moral\": [\"moral\", \"thin morality\", \"non-moral\"],\n",
    "                    \"full\": [\"care\", \"proportionality\", \"loyalty\", \"authority\", \"purity\", \"equality\", \"thin morality\", \"non-moral\"],\n",
    "                    \"complete\": [\"care\", \"harm\", \"equality\", \"proportionality\", \"loyalty\", \"betrayal\", \"authority\", \"subversion\", \"purity\", \"degradation\", \"thin morality\", \"non-moral\"]\n",
    "               }\n",
    "              }\n",
    "\n",
    "foundations_dict = {\n",
    "                    \"full\": {\"harm\": \"care\", \"care\": \"care\", \"degradation\": \"purity\", \n",
    "                            \"purity\": \"purity\", \"betrayal\": \"loyalty\", \"loyalty\": \"loyalty\", \n",
    "                            \"subversion\": \"authority\", \"authority\": \"authority\",\n",
    "                            \"cheating\": \"fairness\", \"fairness\": \"fairness\", \"equality\": \"fairness\",\n",
    "                            \"non-moral\": \"non-moral\", \"nm\": \"non-moral\", \"thin morality\": \"thin morality\", \"proportionality\": \"fairness\"},\n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7a228b-c79f-43e9-b994-e9f5fbbfd75a",
   "metadata": {},
   "source": [
    "General Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3cd3e21c-d845-4dda-9a0c-bae3b1ae8173",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = \"mftc\"\n",
    "mode = \"full\"\n",
    "training = \"cross\"\n",
    "\n",
    "if corp == \"mftc\":\n",
    "    if training == \"cross\":\n",
    "        eval = \"mfrc\"\n",
    "    else:\n",
    "        eval = \"mftc\"\n",
    "elif corp == \"mfrc\":\n",
    "    if training == \"cross\":\n",
    "        eval = \"mftc\"\n",
    "    else:\n",
    "        eval = \"mfrc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a4293-4e0c-4030-9bf4-ab1772a5208d",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ce80b1df-7d0e-4f2e-8e79-3c6156d9b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load samples (groundtruth)\n",
    "if training == \"cross\":\n",
    "    df_groundtruth = pd.read_csv(\"../data/preprocessed/\" + eval + \"_cleaned_\" + mode + \".csv\")\n",
    "    df_meta = pd.read_csv(\"../data/preprocessed/\" + eval + \"_eval_\" + mode + \".csv\")\n",
    "elif training == \"normal\":\n",
    "    df_groundtruth = pd.read_csv(\"../data/preprocessed/\" + corp + \"_sample_\" + mode + \".csv\")   \n",
    "    df_meta = pd.read_csv(\"../data/preprocessed/\" + corp + \"_meta_sample_\" + mode + \".csv\")   \n",
    "else:\n",
    "    pass\n",
    "\n",
    "df_pred = pd.read_csv(\"../results/predictions/\" + corp + \"_labels_\" + training + \"_\" + mode + \".csv\")\n",
    "\n",
    "# load eval data\n",
    "# convert wide to long (convert dummy to label)\n",
    "\n",
    "# analyze\n",
    "    # accuracy per annotator\n",
    "    # regression model: label ~ annotator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253401e3-68dc-4040-b5e8-f131ff6190be",
   "metadata": {},
   "source": [
    "Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5c44e533-64fc-4503-8968-96bab2b59358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat labels for cross corpus classifications (because they are trained on slightly different labels)\n",
    "if training == \"cross\":\n",
    "    if corp == \"mftc\": #if an mftc classifier predicts mfrc -> combine eq and prop to fairness & drop thin morality because the classifier was not trained on these labels\n",
    "        df_groundtruth[\"fairness\"] = (df_groundtruth.equality + df_groundtruth.proportionality) > 0\n",
    "        df_groundtruth.fairness = df_groundtruth.fairness.astraining(int)\n",
    "        df_groundtruth = df_groundtruth.drop([\"thin morality\", \"equality\", \"proportionality\"], axis = 1)\n",
    "        \n",
    "        df_meta.annotation = df_meta.annotation.replace(foundations_dict[mode])\n",
    "        df_meta = df_meta[df_meta.annotation != \"thin morality\"].reset_index(drop=True)\n",
    "    elif corp == \"mfrc\": # if mfrc classifier predicts mftc -> combine eq&prop predictions and drop thin morality predictions because these labels are not in groundtruth\n",
    "        df_pred[\"fairness\"] = (df_pred.equality + df_pred.proportionality) > 0\n",
    "        df_pred.fairness = df_pred.fairness.astraining(int)\n",
    "        df_pred = df_pred.drop([\"thin morality\", \"equality\", \"proportionality\"], axis = 1)\n",
    "        \n",
    "        df_meta.annotation = df_meta.annotation.replace(foundations_dict[mode])\n",
    "        df_meta = df_meta[df_meta.annotation != \"thin morality\"].reset_index(drop=True)\n",
    "    else:\n",
    "        pass\n",
    "elif training == \"normal\":\n",
    "    pass\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d20d42a-942d-48da-a744-b1703763bd8c",
   "metadata": {},
   "source": [
    "Get Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa083fa3-b6d4-4f4c-82de-6cfed0a9ded2",
   "metadata": {},
   "source": [
    "Create dataframes for analyses (Matching of annotator and classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "522a6753-fe46-4622-935f-29c0ebcfd132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4088107417900318\n"
     ]
    }
   ],
   "source": [
    "# for cross domain predictions -> transform to compatible classes (e.g., MFQ1 from MFQ2)\n",
    "if training == \"cross\":\n",
    "    cols = foundations[\"mftc\"][mode]\n",
    "else:\n",
    "    cols = foundations[corp][mode]\n",
    "\n",
    "y_true = df_groundtruth[cols].values\n",
    "y_pred = df_pred[cols]\n",
    "\n",
    "print(f1_score(y_true, y_pred, average=\"macro\"))\n",
    "\n",
    "df_total = df_meta.merge(df_pred[cols + [\"text\"]], on=\"text\")\n",
    "df_total = df_total[df_total.annotation!=\"nh\"].reset_index(drop=True)\n",
    "df_total[\"success\"] = df_total.apply(lambda x: x[x[\"annotation\"]] == 1, axis = 1)\n",
    "\n",
    "df_total.to_csv(\"../results/evals/\" + corp + \"_success_\" + training + \"_\" + mode + \".csv\", index = False) # uncomment for analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d516a7-632a-410f-85d5-bb04a625aab9",
   "metadata": {},
   "source": [
    "### Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550448e0-ec88-4850-9a36-b5cd89b39ae9",
   "metadata": {},
   "source": [
    "Average number of words in texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9541b7ea-b5a1-4831-ac55-46b3340f0555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.88431359399101"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average number of words in texts\n",
    "df_pred.text.str.split(\"\\\\s+\").str.len().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "92a6947e-ab2b-44c9-88c9-04b878386cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  people just WORRIED because they know that Obama and other foreign countries interfered in the French  election\n",
      "\n",
      "Macron can't possibly be seen as legitimate yntil we have an investigation into what Obama did to  help him while president\n",
      "\n",
      "I've heard rumours  Obama might have even used the NSA to spy on le pen  to find out her strategies\n",
      "text                 people just WORRIED because they know that O...\n",
      "care                                                             0.0\n",
      "proportionality                                                    0\n",
      "loyalty                                                          0.0\n",
      "authority                                                        0.0\n",
      "purity                                                           0.0\n",
      "equality                                                           0\n",
      "thin morality                                                      0\n",
      "non-moral                                                        1.0\n",
      "fairness                                                         0.0\n",
      "Name: 15, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# search for entries\n",
    "i = 15\n",
    "print(df_pred.text[i])\n",
    "# print(df_pred.annotation[i])\n",
    "print(df_pred.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c1a3fa-bae7-4f07-a8a7-5e6244760285",
   "metadata": {},
   "source": [
    "Accuracy by annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "451af494-4fea-47f0-bfbf-dd95474146bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "annotator\n",
       "annotator00    0.731435\n",
       "annotator01    0.603389\n",
       "annotator02    0.723866\n",
       "annotator03    0.783196\n",
       "annotator04    0.191821\n",
       "annotator05    0.303060\n",
       "Name: success, dtype: float64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total[\"success\"].groupby(df_total['annotator']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6852ca62-6c6e-494a-a7f8-7b0f894c0272",
   "metadata": {},
   "source": [
    "Distribution of foundations across predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7dd6b2a7-90ef-4bde-9c74-eab8263c4105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "care         0.059409\n",
      "fairness     0.107755\n",
      "loyalty      0.022542\n",
      "authority    0.037052\n",
      "purity       0.022842\n",
      "non-moral    0.758314\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if training == \"cross\":\n",
    "    print(df_total[foundations[\"mftc\"][mode]].sum(0)/df_total.shape[0])\n",
    "else:\n",
    "    print(df_total[foundations[corp][mode]].sum(0)/df_total.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0502c24f-6687-412e-8b49-88b2ba51a7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "care         0.117363\n",
      "fairness     0.105560\n",
      "loyalty      0.032057\n",
      "authority    0.049292\n",
      "purity       0.018309\n",
      "non-moral    0.650526\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# compare distribution of foundations over predictions and groundtruth (are they similar? -> better classifier)\n",
    "if training == \"cross\":\n",
    "    print(df_groundtruth[foundations[\"mftc\"][mode]].sum(0)/df_groundtruth.shape[0])\n",
    "else:\n",
    "    print(df_groundtruth[foundations[corp][mode]].sum(0)/df_groundtruth.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
