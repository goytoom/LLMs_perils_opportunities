{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd468764-85ce-4cb6-8ca3-49700cfb1fb8",
   "metadata": {},
   "source": [
    "This code book calls the OpenAI API to classify moral sentiments in posts from the Moral Foundations Reddit Corpus using fine-tuned ChatGPT!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91abfc0-abfb-4374-a408-6a09fe36c14b",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca7c26cf-9783-4ba8-b0c2-79a8a2d5c4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import string\n",
    "import re\n",
    "remove = string.punctuation\n",
    "remove = remove.replace(\"-\", \"\").replace(\",\", \"\") # don't remove hyphens\n",
    "pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "def count_tokens(text):\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import logging\n",
    "from retry import retry\n",
    "logging.basicConfig()\n",
    "\n",
    "# Calculate the delay based on your rate limit\n",
    "rate_limit_per_minute = 3500.0\n",
    "delay_60 = 60.0 / 60\n",
    "delay_full = 60.0 / rate_limit_per_minute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d4601-a181-4fc0-9ff0-efb3f0d8b0ff",
   "metadata": {},
   "source": [
    "## General Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e40c53c4-f26c-4dc7-babe-39cb77569822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"mfrc\"\n",
    "mode = \"full\"\n",
    "folder = \"../data/preprocessed/\"\n",
    "path = folder + data + \"_sample_\" + mode + \".csv\" # test data\n",
    "path_train = folder + data + \"_train_\" + mode + \".csv\" # train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3f617c43-aca7-4e38-a1ff-8ace4c1b2ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "df_train_total = pd.read_csv(path_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99e6b8-baea-42dc-8467-dafcd97472b8",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d8410c23-d642-40ea-bb96-b6392c4aeec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chatGPT parameters\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") #add your openai key to the environment\n",
    "model_engine = \"gpt-3.5-turbo-0301\"\n",
    "\n",
    "@retry(delay=5)\n",
    "def delayed_completion(delay_in_seconds: float = 1, **kwargs):\n",
    "    \"\"\"Delay a completion by a specified amount of time.\"\"\"\n",
    "\n",
    "    # Sleep for the delay\n",
    "    time.sleep(delay_in_seconds)\n",
    "\n",
    "    # Call the Completion API and return the result\n",
    "    return openai.ChatCompletion.create(**kwargs)\n",
    "\n",
    "def separate_labels(df, cols):\n",
    "    def _set_labels(row):\n",
    "        for label in row[\"annotations\"].split(\",\"):\n",
    "            if label in cols:\n",
    "                row[label.strip()] = 1\n",
    "        return row\n",
    "\n",
    "    # removing texts with no annotations\n",
    "    df = df[df.annotations != ''].reset_index(drop=True)\n",
    "    df = df[~ pd.isna(df.annotations)].reset_index(drop=True)\n",
    "    for label in cols:\n",
    "        df[label] = 0\n",
    "    df = df.apply(_set_labels, axis=1).drop([\"annotations\"], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75469f4-5b5f-4463-bef5-1f325d533c7d",
   "metadata": {},
   "source": [
    "## Load Raw Data and Generate Prompts for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b0766258-83c5-442f-82e4-b5b9e67fcfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts(df):\n",
    "    def concat_column_names(row):\n",
    "        response = ', '.join([col for col in df.columns[1:-1] if row[col] == 1])\n",
    "        if not response:\n",
    "            response = \"non-moral\"\n",
    "        return response\n",
    "    df['responses'] = df.apply(concat_column_names, axis=1)\n",
    "\n",
    "    prompts = []\n",
    "    for (text, response) in zip(df.text, df.responses):\n",
    "        prompts.append({\"messages\":[\n",
    "            {\"role\": \"system\", \"content\": INSTR_TEXT},\n",
    "            {\"role\": \"user\", \"content\": USER_TEXT + text},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ]})\n",
    "\n",
    "    return prompts\n",
    "\n",
    "# System role instructions\n",
    "INSTR_TEXT = \"Detect the presence of moral sentiments in a text based on their definitions.\"\n",
    "USER_TEXT = \"Determine which moral sentiments are expressed in the following text. The text contains \"\\\n",
    "\"\\\"care\\\" if the text is about avoiding emotional and physical damage to another individual, \" \\\n",
    "\"\\\"equality\\\" if the text is about equal treatment and equal outcome for individuals, \" \\\n",
    "\"\\\"proportionality\\\" if the text is about individuals getting rewarded in proportion to their merit or contribution, \"\\\n",
    "\"\\\"loyalty\\\" if the text is about cooperating with ingroups and competing with outgroups, \"\\\n",
    "\"\\\"authority\\\" if the text is about deference toward legitimate authorities and the defense of traditions, \"\\\n",
    "\"all of which are seen as providing stability and fending off chaos, \"\\\n",
    "\"\\\"purity\\\" if the text is about avoiding bodily and spiritual contamination and degradation, \"\\\n",
    "\"\\\"thin morality\\\" if the text has a moral sentiment but cannot be categorized as either of the above. \"\\\n",
    "\"Respond only with these words. Respond with all words that apply, comma separated. Here is the text: \" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8221bf81-c5ee-4a28-96ca-90fc28049e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate prompts\n",
    "df_train, df_val = train_test_split(df_train_total, random_state=0, test_size=0.2, stratify=df_train_total[\"non-moral\"])\n",
    "prompts_train = generate_prompts(df_train.copy())\n",
    "prompts_val = generate_prompts(df_val.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d984fc06-d97c-4770-99e7-7d2062ed3b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'Detect the presence of moral sentiments in a text based on their definitions.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Determine which moral sentiments are expressed in the following text. The text contains \"care\" if the text is about avoiding emotional and physical damage to another individual, \"equality\" if the text is about equal treatment and equal outcome for individuals, \"proportionality\" if the text is about individuals getting rewarded in proportion to their merit or contribution, \"loyalty\" if the text is about cooperating with ingroups and competing with outgroups, \"authority\" if the text is about deference toward legitimate authorities and the defense of traditions, all of which are seen as providing stability and fending off chaos, \"purity\" if the text is about avoiding bodily and spiritual contamination and degradation, \"thin morality\" if the text has a moral sentiment but cannot be categorized as either of the above. Respond only with these words. Respond with all words that apply, comma separated. Here is the text: I can smell and taste this image but my brain is not connecting with my past! I don\\'t know how I knew this dog!'},\n",
       "  {'role': 'assistant', 'content': 'non-moral'}]}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### check prompt\n",
    "prompts_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b7482bb-f788-44bd-92e6-e19695c4ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tuning data:\n",
    "data_path_train = \"../data/moral_fine_tuning_train.jsonl\"\n",
    "with open(data_path_train, \"w\", encoding='utf-8') as f:\n",
    "    for prompt in prompts_train:\n",
    "        json.dump(prompt, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "data_path_val = \"../data/moral_fine_tuning_val.jsonl\"\n",
    "with open(data_path_val, \"w\", encoding='utf-8') as f:\n",
    "    for prompt in prompts_val:\n",
    "        json.dump(prompt, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cffbc4-9d69-4a57-9b40-5e486f3dfcfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data validation and cost estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e9357ff-5761-42d9-a4bc-ca9e2d24ceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 9542\n",
      "First example:\n",
      "{'role': 'system', 'content': 'Detect the presence of moral sentiments in a text based on their definitions.'}\n",
      "{'role': 'user', 'content': 'Determine which moral sentiments are expressed in the following text. The text contains \"care\" if the text is about avoiding emotional and physical damage to another individual, \"equality\" if the text is about equal treatment and equal outcome for individuals, \"proportionality\" if the text is about individuals getting rewarded in proportion to their merit or contribution, \"loyalty\" if the text is about cooperating with ingroups and competing with outgroups, \"authority\" if the text is about deference toward legitimate authorities and the defense of traditions, all of which are seen as providing stability and fending off chaos, \"purity\" if the text is about avoiding bodily and spiritual contamination and degradation, \"thin morality\" if the text has a moral sentiment but cannot be categorized as either of the above. Respond only with these words. Respond with all words that apply, comma separated. Here is the text: I can smell and taste this image but my brain is not connecting with my past! I don\\'t know how I knew this dog!'}\n",
      "{'role': 'assistant', 'content': 'non-moral'}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_path = \"../data/moral_fine_tuning_train.jsonl\"\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "963e349c-ad96-41c0-89fc-f8037c79c0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afa749e5-49ac-4e9c-8321-9cc754682fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "082046b2-a1b8-4bda-9f82-b1cbee4ea4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 221, 538\n",
      "mean / median: 255.04946552085516, 246.0\n",
      "p5 / p95: 228.0, 298.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 1, 10\n",
      "mean / median: 2.6193670090127856, 3.0\n",
      "p5 / p95: 1.0, 3.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b164c3bd-a2f0-413b-9427-e9e9326881cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~2433682 tokens that will be charged for during training\n",
      "By default, you'll train for 2 epochs on this dataset\n",
      "By default, you'll be charged for ~4867364 tokens\n",
      "By default, you'll be charged ~38.94$\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "print(f\"By default, you'll be charged ~{round(n_epochs * n_billing_tokens_in_dataset * 0.008/1000, 2)}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff20a87-56bf-49b0-b712-3289f49fd425",
   "metadata": {},
   "source": [
    "## Fine-tune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1261984c-6536-41ad-bf08-7b1fac19eab3",
   "metadata": {},
   "source": [
    "### Upload Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c0ba4ff-483b-4b2c-ab5d-59eeaa701c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<File file id=file-45wVWmAaT8Ct7tJT110vCFhc at 0x7f196808c770> JSON: {\n",
       "  \"object\": \"file\",\n",
       "  \"id\": \"file-45wVWmAaT8Ct7tJT110vCFhc\",\n",
       "  \"purpose\": \"fine-tune\",\n",
       "  \"filename\": \"file\",\n",
       "  \"bytes\": 12831924,\n",
       "  \"created_at\": 1697820790,\n",
       "  \"status\": \"uploaded\",\n",
       "  \"status_details\": null\n",
       "}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.File.create(\n",
    "  file=open(data_path_train, \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb6e4d26-37c1-4451-a358-5429f593bebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<File file id=file-PwQlW6vqTDmsLZjkdrHNGqGe at 0x7f191c4c93a0> JSON: {\n",
       "  \"object\": \"file\",\n",
       "  \"id\": \"file-PwQlW6vqTDmsLZjkdrHNGqGe\",\n",
       "  \"purpose\": \"fine-tune\",\n",
       "  \"filename\": \"file\",\n",
       "  \"bytes\": 3224234,\n",
       "  \"created_at\": 1697820847,\n",
       "  \"status\": \"uploaded\",\n",
       "  \"status_details\": null\n",
       "}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.File.create(\n",
    "  file=open(data_path_val, \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e44066-c042-41d6-99c2-d02c85790fff",
   "metadata": {},
   "source": [
    "### Create Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "60b6f0a7-e3c1-4fe1-8bd1-b15ca1fb5559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-waCyJ9TOZ6ep5OHcrN10fQj4 at 0x7f191e182610> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-waCyJ9TOZ6ep5OHcrN10fQj4\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1697821545,\n",
       "  \"finished_at\": null,\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"organization_id\": \"org-yVS6iCwS4TFCckmhlIwrbL8m\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"validating_files\",\n",
       "  \"validation_file\": \"file-PwQlW6vqTDmsLZjkdrHNGqGe\",\n",
       "  \"training_file\": \"file-45wVWmAaT8Ct7tJT110vCFhc\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 2\n",
       "  },\n",
       "  \"trained_tokens\": null,\n",
       "  \"error\": null\n",
       "}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.FineTuningJob.create(training_file=\"file-45wVWmAaT8Ct7tJT110vCFhc\", model=\"gpt-3.5-turbo-0613\", suffix=\"mfrc_tuned\", validation_file=\"file-PwQlW6vqTDmsLZjkdrHNGqGe\", hyperparameters={\"n_epochs\":2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a530845-3eaa-49d0-a1c8-7aca4e8e1c91",
   "metadata": {},
   "source": [
    "### Check up on job (or use online interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "61e57b5c-48ad-4c3c-86fd-2cd99b854c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-waCyJ9TOZ6ep5OHcrN10fQj4 at 0x7f1916a2c360> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-waCyJ9TOZ6ep5OHcrN10fQj4\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1697821545,\n",
       "  \"finished_at\": null,\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"organization_id\": \"org-yVS6iCwS4TFCckmhlIwrbL8m\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"validating_files\",\n",
       "  \"validation_file\": \"file-PwQlW6vqTDmsLZjkdrHNGqGe\",\n",
       "  \"training_file\": \"file-45wVWmAaT8Ct7tJT110vCFhc\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 2\n",
       "  },\n",
       "  \"trained_tokens\": null,\n",
       "  \"error\": null\n",
       "}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List 10 fine-tuning jobs\n",
    "openai.FineTuningJob.list(limit=10)\n",
    "\n",
    "# # # Retrieve the state of a fine-tune\n",
    "# openai.FineTuningJob.retrieve(\"ftjob-waCyJ9TOZ6ep5OHcrN10fQj4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6966acd3-3099-47c9-9fff-67fa40c8bc1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Try fine-tuning API on small subset first (only for test purposes if problems occur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4395e78b-6bbc-40af-9391-0c98724fbdde",
   "metadata": {},
   "source": [
    "Use this to test if the fine-tuning service works on a small dataset if necessary (avoids paying for the large training data in case errors occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0b61b895-8bbd-4f82-9c95-483b484f2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['strata'] = df_train[df_train.columns[1:-1]].apply(lambda row: '_'.join(row.map(str)), axis=1)\n",
    "df_val['strata'] = df_val[df_val.columns[1:-1]].apply(lambda row: '_'.join(row.map(str)), axis=1)\n",
    "\n",
    "# Sample from each stratum\n",
    "samples = []\n",
    "for stratum, group in df_train.groupby('strata'):\n",
    "    samples.append(group.sample(frac=0.01, random_state=0))\n",
    "df_train_mini = pd.concat(samples, axis=0)\n",
    "\n",
    "samples = []\n",
    "for stratum, group in df_val.groupby('strata'):\n",
    "    samples.append(group.sample(frac=0.01, random_state=0))\n",
    "df_val_mini = pd.concat(samples, axis=0)\n",
    "\n",
    "# Drop the strata column if you don't need it anymore\n",
    "df_train_mini = df_train_mini.drop(columns=['strata'])\n",
    "df_val_mini = df_val_mini.drop(columns=['strata'])\n",
    "\n",
    "prompts_train_mini = generate_prompts(df_train_mini.copy())\n",
    "prompts_val_mini = generate_prompts(df_val_mini.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "46910043-5e39-43ed-9e31-52283997b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save mini tuning data:\n",
    "data_path_train_mini = \"../data/moral_fine_tuning_train_mini.jsonl\"\n",
    "with open(data_path_train_mini, \"w\", encoding='utf-8') as f:\n",
    "    for prompt in prompts_train_mini:\n",
    "        json.dump(prompt, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "data_path_val_mini = \"../data/moral_fine_tuning_val_mini.jsonl\"\n",
    "with open(data_path_val_mini, \"w\", encoding='utf-8') as f:\n",
    "    for prompt in prompts_val_mini:\n",
    "        json.dump(prompt, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dcaf2044-2fbe-46b1-bde9-3cb6947ddb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<File file id=file-AeTDS2IoaFK2YBbaesSyHcGE at 0x7f191e1265c0> JSON: {\n",
       "  \"object\": \"file\",\n",
       "  \"id\": \"file-AeTDS2IoaFK2YBbaesSyHcGE\",\n",
       "  \"purpose\": \"fine-tune\",\n",
       "  \"filename\": \"file\",\n",
       "  \"bytes\": 124562,\n",
       "  \"created_at\": 1697825882,\n",
       "  \"status\": \"uploaded\",\n",
       "  \"status_details\": null\n",
       "}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.File.create(\n",
    "  file=open(data_path_train_mini, \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fbfe12b6-4df5-4cd1-9100-6b068820f65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<File file id=file-meIj4wsewNZvZtWCCRX0CVtF at 0x7f191e10b650> JSON: {\n",
       "  \"object\": \"file\",\n",
       "  \"id\": \"file-meIj4wsewNZvZtWCCRX0CVtF\",\n",
       "  \"purpose\": \"fine-tune\",\n",
       "  \"filename\": \"file\",\n",
       "  \"bytes\": 28245,\n",
       "  \"created_at\": 1697825883,\n",
       "  \"status\": \"uploaded\",\n",
       "  \"status_details\": null\n",
       "}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.File.create(\n",
    "  file=open(data_path_val_mini, \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7117ec4f-c3c4-435b-b81b-082acb72a573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-wD3oTJBibcWF1VxmEx9Hzsol at 0x7f191a8fe5c0> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-wD3oTJBibcWF1VxmEx9Hzsol\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1697826444,\n",
       "  \"finished_at\": null,\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"organization_id\": \"org-yVS6iCwS4TFCckmhlIwrbL8m\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"validating_files\",\n",
       "  \"validation_file\": \"file-meIj4wsewNZvZtWCCRX0CVtF\",\n",
       "  \"training_file\": \"file-AeTDS2IoaFK2YBbaesSyHcGE\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 3\n",
       "  },\n",
       "  \"trained_tokens\": null,\n",
       "  \"error\": null\n",
       "}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.FineTuningJob.create(training_file=\"file-AeTDS2IoaFK2YBbaesSyHcGE\", model=\"gpt-3.5-turbo-0613\", suffix=\"mfrc_tuned_mini\", validation_file=\"file-meIj4wsewNZvZtWCCRX0CVtF\", hyperparameters={\"n_epochs\":3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd565e7-163d-4a36-b6b0-2728c3bd6246",
   "metadata": {},
   "source": [
    "# Run Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6caedb6f-8cf5-40e9-a03a-cf8b9682b54b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2983, 9)\n",
      "2983\n"
     ]
    }
   ],
   "source": [
    "# load annotation texts\n",
    "df_test = pd.read_csv(path)\n",
    "print(df_test.shape)\n",
    "prompts_test_full = generate_prompts(df_test.copy())\n",
    "prompts_test = [prompt[\"messages\"][:-1] for prompt in prompts_test_full]\n",
    "print(len(prompts_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "dd7f367f-0bf7-4393-8d4e-ed4dee80813e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'Detect the presence of moral sentiments in a text based on their definitions.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Determine which moral sentiments are expressed in the following text. The text contains \"care\" if the text is about avoiding emotional and physical damage to another individual, \"equality\" if the text is about equal treatment and equal outcome for individuals, \"proportionality\" if the text is about individuals getting rewarded in proportion to their merit or contribution, \"loyalty\" if the text is about cooperating with ingroups and competing with outgroups, \"authority\" if the text is about deference toward legitimate authorities and the defense of traditions, all of which are seen as providing stability and fending off chaos, \"purity\" if the text is about avoiding bodily and spiritual contamination and degradation, \"thin morality\" if the text has a moral sentiment but cannot be categorized as either of the above. Respond only with these words. Respond with all words that apply, comma separated. Here is the text: Yes, it\\'s understandable for the victims\\' loved ones to be in immense pain, and it\\'s a genuine irreparable tragedy. But that\\'s also why we don\\'t let the families of victims decide what happens to the offenders if we want to have a nation of impartial justice and law.'},\n",
       " {'role': 'assistant', 'content': 'care'}]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check prompt\n",
    "prompts_test_full[2][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "62f9e856-cb8d-4602-a18f-8ca19c86ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tuned model\n",
    "tuned_model =\"ft:gpt-3.5-turbo-0613:personal:mfrc-tuned:8Bpv4Ow4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d9776-c62e-448d-a7a7-f4cab94b15dd",
   "metadata": {},
   "source": [
    "## Test Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "52250c34-9345-4845-9042-8c772c56e102",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thin morality\n"
     ]
    }
   ],
   "source": [
    "APIresponse = delayed_completion(\n",
    "    delay_in_seconds=delay_full,\n",
    "    model=tuned_model,\n",
    "    messages=prompts_test[0],\n",
    "    temperature=0\n",
    "    )\n",
    "response = APIresponse.choices[0].message[\"content\"]\n",
    "print(response) #works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce67b9f-4dbc-4ffb-9e66-bbf82f6881f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c57ca5-e53a-4bef-bce8-db79611d7536",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses = []\n",
    "for i, prompt in enumerate(prompts_test):\n",
    "    APIresponse = delayed_completion(\n",
    "        delay_in_seconds=delay_full,\n",
    "        model=tuned_model,\n",
    "        messages=prompt,\n",
    "        temperature=0,\n",
    "        )\n",
    "    response = APIresponse.choices[0].message[\"content\"]\n",
    "    responses.append(response)\n",
    "    if not i % int(0.1 * len(prompts_test)):\n",
    "        print(str(int(i/ len(prompts_test)*100)) + \"\\%\")\n",
    "\n",
    "# clean gpt outputs (for predictions that have imprecise wording, e.g., none for non-moral)\n",
    "responses_cleaned = [re.sub(pattern, \"\", x.lower()) if \"non\" not in x.lower() else \"non-moral\" for x in responses]\n",
    "\n",
    "# save as dataframe\n",
    "new_dic = {}\n",
    "new_dic[\"text\"] = df_test.text.tolist()\n",
    "new_dic[\"annotations_raw\"] = responses\n",
    "df_responses = pd.DataFrame(new_dic)\n",
    "df_responses.to_csv(\"../results/predictions/gpt_FT_\" + data + \"_labels_\" + mode + \"_raw.csv\", index=False)\n",
    "\n",
    "df_responses_cleaned = df_responses.drop([\"annotations_raw\"], axis=1).copy()\n",
    "df_responses_cleaned[\"annotations\"] = responses_cleaned\n",
    "\n",
    "cols = df.columns[1:].tolist()\n",
    "df_preds = separate_labels(df_responses_cleaned, cols)\n",
    "df_preds.to_csv(\"../results/predictions/gpt_FT_\" + data + \"_labels_\" + mode + \".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
