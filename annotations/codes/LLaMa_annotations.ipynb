{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e81fb8-6285-4c25-bcc1-f4faa90e8c94",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ca7c26cf-9783-4ba8-b0c2-79a8a2d5c4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "import string\n",
    "import re\n",
    "remove = string.punctuation\n",
    "remove = remove.replace(\"-\", \"\").replace(\",\", \"\") # don't remove hyphens\n",
    "pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import logging\n",
    "from retry import retry\n",
    "logging.basicConfig()\n",
    "\n",
    "# Calculate the delay based on your rate limit\n",
    "rate_limit_per_minute = 5000.0\n",
    "delay_60 = 60.0 / 60\n",
    "delay_full = 60.0 / rate_limit_per_minute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d4601-a181-4fc0-9ff0-efb3f0d8b0ff",
   "metadata": {},
   "source": [
    "## General Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40c53c4-f26c-4dc7-babe-39cb77569822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"mfrc\"\n",
    "mode = \"full\"\n",
    "folder = \"../data/preprocessed/\"\n",
    "path = folder + data + \"_sample_\" + mode + \".csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99e6b8-baea-42dc-8467-dafcd97472b8",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d8410c23-d642-40ea-bb96-b6392c4aeec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chatGPT parameters\n",
    "HOST = 'localhost:5000'\n",
    "URI = f'http://{HOST}/api/v1/generate'\n",
    "\n",
    "@retry(delay=5)\n",
    "def run(prompt, verbose=0, slow_down=0.001):\n",
    "    request = {\n",
    "        'prompt': prompt,\n",
    "        'max_new_tokens': 150,\n",
    "        'mode' : 'instruct',\n",
    "\n",
    "        # Generation params. If 'preset' is set to different than 'None', the values\n",
    "        # in presets/preset-name.yaml are used instead of the individual numbers.\n",
    "        'preset': 'None',\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.01,\n",
    "        'top_p': 0.14,\n",
    "        'typical_p': 1,\n",
    "        'epsilon_cutoff': 0,  # In units of 1e-4\n",
    "        'eta_cutoff': 0,  # In units of 1e-4\n",
    "        'tfs': 1,\n",
    "        'top_a': 0,\n",
    "        'repetition_penalty': 1.17,\n",
    "        'repetition_penalty_range': 0,\n",
    "        'encoder_repetition_penalty': 1,\n",
    "        'top_k': 49,\n",
    "        'min_length': 0,\n",
    "        'no_repeat_ngram_size': 0,\n",
    "        'num_beams': 1,\n",
    "        'penalty_alpha': 0,\n",
    "        'length_penalty': 1,\n",
    "        'early_stopping': False,\n",
    "        'mirostat_mode': 0,\n",
    "        'mirostat_tau': 5,\n",
    "        'mirostat_eta': 0.1,\n",
    "        # 'instruction_template': \"Instruct-Alpaca\",\n",
    "\n",
    "        'seed': -1,\n",
    "        'add_bos_token': True,\n",
    "        'truncation_length': 2048,\n",
    "        'ban_eos_token': False,\n",
    "        'skip_special_tokens': True,\n",
    "        'stopping_strings': []\n",
    "    }\n",
    "\n",
    "    response = requests.post(URI, json=request)\n",
    "\n",
    "    if response.status_code == 200 and verbose == 1:\n",
    "        result = response.json()['results'][0]['text']\n",
    "        print(prompt + result)\n",
    "    time.sleep(slow_down)\n",
    "    return response\n",
    "\n",
    "def model_api(request):\n",
    "    response = requests.post(f'http://{HOST}/api/v1/model', json=request)\n",
    "    return response.json()\n",
    "\n",
    "def model_info():\n",
    "    response = model_api({'action': 'info'})\n",
    "    print_basic_model_info(response)\n",
    "\n",
    "def print_basic_model_info(response):\n",
    "    basic_settings = ['truncation_length', 'instruction_template']\n",
    "    print(\"Model: \", response['result']['model_name'])\n",
    "    print(\"Lora(s): \", response['result']['lora_names'])\n",
    "    for setting in basic_settings:\n",
    "        print(setting, \"=\", response['result']['shared.settings'][setting])\n",
    "\n",
    "def separate_labels(df, cols):\n",
    "    def _set_labels(row):\n",
    "        for label in row[\"annotations\"].split(\",\"):\n",
    "            if label.strip() in cols:\n",
    "                row[label.strip()] = 1\n",
    "        return row\n",
    "\n",
    "    # removing texts with no annotations\n",
    "    df = df[df.annotations != ''].reset_index(drop=True)\n",
    "    df = df[~ pd.isna(df.annotations)].reset_index(drop=True)\n",
    "    for label in cols:\n",
    "        df[label] = 0\n",
    "    df = df.apply(_set_labels, axis=1).drop([\"annotations\"], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9384f743-4d22-4464-ae81-41cc3d89e742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  TheBloke_Luna-AI-Llama2-Uncensored-GPTQ_gptq-4bit-32g-actorder_True\n",
      "Lora(s):  []\n",
      "truncation_length = 2048\n",
      "instruction_template = None\n"
     ]
    }
   ],
   "source": [
    "model_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75469f4-5b5f-4463-bef5-1f325d533c7d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bd822b6e-7bbf-4920-8fe5-678d5118e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEXT_1 = \"USER: These are definitions of moral sentiments: \"\\\n",
    "\"\\\"care\\\" if a text is about avoiding emotional and physical damage to another individual, \\\"equality\\\" if a text is about equal treatment and equal outcome for individuals, \"\\\n",
    "\"\\\"proportionality\\\" if a text is about individuals getting rewarded in proportion to their merit or contribution, \\\"loyalty\\\" if a text is about cooperating with ingroups and competing with outgroups, \"\\\n",
    "\"\\\"authority\\\" if a text is about deference toward legitimate authorities and the defense of traditions, all of which are seen as providing stability and fending off chaos, \"\\\n",
    "\"\\\"purity\\\" if a text is about avoiding bodily and spiritual contamination and degradation, \\\"thin-morality\\\" if a text has a moral sentiment but cannot be categorized as either of the above, \"\\\n",
    "\"\\\"none\\\" if no moral sentiment is expressed in the text.\"\\\n",
    "\"\\n\\nBased solely on these definitions, name all moral sentiments that are directly expressed in the following text:\\n\"\\\n",
    "\"\\\"\\\"\\\"\"\n",
    "\n",
    "PROMPT_TEXT_2 = \"\\\"\\\"\\\"\\n\\nReturn a comma separated list of all moral sentiments that were expressed in the text.\\n\\nASSISTANT:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f7a08fa9-89bc-4896-8451-50e221d3cd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: These are definitions of moral sentiments: \"care\" if a text is about avoiding emotional and physical damage to another individual, \"equality\" if a text is about equal treatment and equal outcome for individuals, \"proportionality\" if a text is about individuals getting rewarded in proportion to their merit or contribution, \"loyalty\" if a text is about cooperating with ingroups and competing with outgroups, \"authority\" if a text is about deference toward legitimate authorities and the defense of traditions, all of which are seen as providing stability and fending off chaos, \"purity\" if a text is about avoiding bodily and spiritual contamination and degradation, \"thin-morality\" if a text has a moral sentiment but cannot be categorized as either of the above, \"none\" if no moral sentiment is expressed in the text.\n",
      "\n",
      "Based on this definition, name all moral sentiments that are expressed in the following text:\n",
      "\"\"\"\n"
     ]
    }
   ],
   "source": [
    "print(PROMPT_TEXT_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "87b1718a-ea92-4df9-beb4-20687d5bbe9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "\n",
      "Return a comma separated list of all moral sentiments that were expressed in the text.\n",
      "\n",
      "ASSISTANT:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(PROMPT_TEXT_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6caedb6f-8cf5-40e9-a03a-cf8b9682b54b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2983, 9)\n",
      "33\n",
      "USER: These are definitions of moral sentiments: \"care\" if a text is about avoiding emotional and physical damage to another individual, \"equality\" if a text is about equal treatment and equal outcome for individuals, \"proportionality\" if a text is about individuals getting rewarded in proportion to their merit or contribution, \"loyalty\" if a text is about cooperating with ingroups and competing with outgroups, \"authority\" if a text is about deference toward legitimate authorities and the defense of traditions, all of which are seen as providing stability and fending off chaos, \"purity\" if a text is about avoiding bodily and spiritual contamination and degradation, \"thin-morality\" if a text has a moral sentiment but cannot be categorized as either of the above, \"none\" if no moral sentiment is expressed in the text.\n",
      "\n",
      "Based solely on these definitions, name all moral sentiments that are directly expressed in the following text:\n",
      "\"\"\"Was just browsing r/Politics, noticed [this tired old trope](https://www.reddit.com/r/politics/comments/6goce7/bernie_sanders_says_labour_party_shows_the_way_to/dirtr7d/) rearing it's ugly head. DAE Macron is actually left for American politics?\"\"\"\n",
      "\n",
      "Return a comma separated list of all moral sentiments that were expressed in the text.\n",
      "\n",
      "ASSISTANT:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(path)\n",
    "print(df.shape)\n",
    "print(round(df.text.str.split(\"\\\\s+\").str.len().mean()))\n",
    "prompts = [PROMPT_TEXT_1 + x + PROMPT_TEXT_2 for x in df.text]\n",
    "print(prompts[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d9776-c62e-448d-a7a7-f4cab94b15dd",
   "metadata": {},
   "source": [
    "## Test Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a5ece223-9a5b-40b4-9f79-542e14540337",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: These are definitions of moral sentiments: \"care\" if a text is about avoiding emotional and physical damage to another individual, \"equality\" if a text is about equal treatment and equal outcome for individuals, \"proportionality\" if a text is about individuals getting rewarded in proportion to their merit or contribution, \"loyalty\" if a text is about cooperating with ingroups and competing with outgroups, \"authority\" if a text is about deference toward legitimate authorities and the defense of traditions, all of which are seen as providing stability and fending off chaos, \"purity\" if a text is about avoiding bodily and spiritual contamination and degradation, \"thin-morality\" if a text has a moral sentiment but cannot be categorized as either of the above, \"none\" if no moral sentiment is expressed in the text.\n",
      "\n",
      "Based solely on these definitions, name all moral sentiments that are directly expressed in the following text:\n",
      "\"\"\"Yes, it's understandable for the victims' loved ones to be in immense pain, and it's a genuine irreparable tragedy. But that's also why we don't let the families of victims decide what happens to the offenders if we want to have a nation of impartial justice and law.\"\"\"\n",
      "\n",
      "Return a comma separated list of all moral sentiments that were expressed in the text.\n",
      "\n",
      "ASSISTANT:\n",
      "care, loyalty\n"
     ]
    }
   ],
   "source": [
    "test_prompt = prompts[2]\n",
    "answer_test = run(test_prompt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce67b9f-4dbc-4ffb-9e66-bbf82f6881f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8e1e918f-08bb-4d56-a15a-75372d5bc360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['care', 'proportionality', 'loyalty', 'authority', 'purity', 'equality',\n",
       "       'thin morality', 'non-moral'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a5c57ca5-e53a-4bef-bce8-db79611d7536",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt_style' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m cols \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     17\u001b[0m df_preds \u001b[38;5;241m=\u001b[39m separate_labels(df_responses, cols)\n\u001b[1;32m---> 18\u001b[0m df_preds\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../results/predictions/llama2_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m data \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_labels_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m mode \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mprompt_style\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prompt_style' is not defined"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for i, prompt in enumerate(prompts):\n",
    "    APIresponse = run(prompt, 0, delay_full)\n",
    "    response = APIresponse.json()[\"results\"][0][\"text\"]\n",
    "    responses.append(response)\n",
    "\n",
    "# define categories to be found in llama output\n",
    "foundations = [\"care\", \"equality\", \"proportionality\", \"loyalty\", \"authority\", \"purity\", \"thin-morality\"]\n",
    "# clean gpt outputs (for predictions that have imprecise wording, e.g., punctuation)\n",
    "responses_cleaned = [re.sub(pattern, \"\", x.lower()) for x in responses]\n",
    "# responses_cleaned = [x if \"no moral sentiments\" not in x.lower() else \"non-moral\" for x in responses_cleaned]\n",
    "responsesToFoundations = [list(set([y for y in foundations if y in x])) for x in responses_cleaned] #find foundation names in cleaned strings\n",
    "responsesToFoundations = [[\"non-moral\"] if not x else x for x in responsesToFoundations] # no foundation = non-moral\n",
    "responsesToFoundations = [\",\".join(x) for x in responsesToFoundations] #reformat for further processing\n",
    "\n",
    "new_dic = {}\n",
    "new_dic[\"text\"] = df.text.tolist()\n",
    "new_dic[\"annotations\"] = responsesToFoundations\n",
    "df_responses = pd.DataFrame(new_dic)\n",
    "\n",
    "cols = df.columns[1:].tolist()\n",
    "df_preds = separate_labels(df_responses, cols) # format to final dataset\n",
    "df_preds.to_csv(\"../results/predictions/llama2_\" + data + \"_labels_\" + mode + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7d7fa-aead-40d1-82ae-2fc7f1dc85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to find foundations -> save as unique list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1f02af96-c7fa-4b0d-a584-8c7d7fe77315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foundations = [\"care\", \"equality\", \"proportionality\", \"loyalty\", \"authority\", \"purity\", \"thin-morality\"]\n",
    "# responses_cleaned = [re.sub(pattern, \"\", x.lower()) for x in responses]\n",
    "# # responses_cleaned = [x if \"no moral sentiments\" not in x.lower() else \"non-moral\" for x in responses_cleaned]\n",
    "# responsesToFoundations = [list(set([y for y in foundations if y in x])) for x in responses_cleaned]\n",
    "# responsesToFoundations = [[\"non-moral\"] if not x else x for x in responsesToFoundations]\n",
    "# responsesToFoundations = [\",\".join(x) for x in responsesToFoundations]\n",
    "\n",
    "# new_dic = {}\n",
    "# new_dic[\"text\"] = df.text.tolist()\n",
    "# new_dic[\"annotations\"] = responsesToFoundations\n",
    "# df_responses = pd.DataFrame(new_dic)\n",
    "\n",
    "# cols = df.columns[1:].tolist()\n",
    "# df_preds = separate_labels(df_responses, cols)\n",
    "df_preds.to_csv(\"../results/predictions/llama2_\" + data + \"_labels_\" + mode + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1d7e7ccc-a927-4780-bafb-1b577735cad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>care</th>\n",
       "      <th>proportionality</th>\n",
       "      <th>loyalty</th>\n",
       "      <th>authority</th>\n",
       "      <th>purity</th>\n",
       "      <th>equality</th>\n",
       "      <th>thin morality</th>\n",
       "      <th>non-moral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. That's\\n2. Why\\n3. Macron\\n4. Won\\n5. Bitch</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yeah that was my point. Maybe Hillary is more ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes, it's understandable for the victims' love...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Duh we all know you can’t have black friends a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'd bin them to be fair.\\n\\nNo amount of deter...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Le Pen is now a meme. She was really bad.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&amp;gt;I've seen enough. Raphael Warnock (D) defe...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>No.\\nShe's lying about everything, don't be in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FL here and waited in a long line to vote red ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I too would like pictures of Trudeau and Macro...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Was just browsing r/Politics, noticed [this ti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I don't actually believe you, but I'll formula...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&amp;gt; Just so all of you know, french media is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Every fucking time I use indeed I get a millio...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MY FELLOW AMERICANS I WOULD LIKE TO ONCE AGAIN...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  care  proportionality  \\\n",
       "0      1. That's\\n2. Why\\n3. Macron\\n4. Won\\n5. Bitch     0                0   \n",
       "1   Yeah that was my point. Maybe Hillary is more ...     0                0   \n",
       "2   Yes, it's understandable for the victims' love...     1                0   \n",
       "3   Duh we all know you can’t have black friends a...     0                1   \n",
       "4   I'd bin them to be fair.\\n\\nNo amount of deter...     0                0   \n",
       "5           Le Pen is now a meme. She was really bad.     0                0   \n",
       "6   &gt;I've seen enough. Raphael Warnock (D) defe...     1                1   \n",
       "7   No.\\nShe's lying about everything, don't be in...     0                0   \n",
       "8   FL here and waited in a long line to vote red ...     0                0   \n",
       "9   I too would like pictures of Trudeau and Macro...     0                0   \n",
       "10  Was just browsing r/Politics, noticed [this ti...     0                0   \n",
       "11  I don't actually believe you, but I'll formula...     0                0   \n",
       "12  &gt; Just so all of you know, french media is ...     1                0   \n",
       "13  Every fucking time I use indeed I get a millio...     1                0   \n",
       "14  MY FELLOW AMERICANS I WOULD LIKE TO ONCE AGAIN...     0                0   \n",
       "\n",
       "    loyalty  authority  purity  equality  thin morality  non-moral  \n",
       "0         0          0       0         0              0          1  \n",
       "1         0          0       0         0              0          1  \n",
       "2         1          0       0         0              0          0  \n",
       "3         0          0       0         0              0          0  \n",
       "4         0          0       0         0              0          1  \n",
       "5         0          0       0         0              0          1  \n",
       "6         1          1       1         1              0          0  \n",
       "7         0          0       0         0              0          1  \n",
       "8         0          0       0         0              0          1  \n",
       "9         0          0       0         0              0          1  \n",
       "10        0          0       0         0              0          1  \n",
       "11        0          0       0         0              0          1  \n",
       "12        1          0       0         0              0          0  \n",
       "13        1          0       0         0              0          0  \n",
       "14        0          0       0         0              0          1  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preds.head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
