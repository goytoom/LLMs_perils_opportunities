{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c87d544a-e1a0-4da7-96f4-df8d73b107d7",
   "metadata": {},
   "source": [
    "This codebook annotates the moral sentiment of the MFRC test sample using a llama 2 model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e81fb8-6285-4c25-bcc1-f4faa90e8c94",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca7c26cf-9783-4ba8-b0c2-79a8a2d5c4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "import string\n",
    "import re\n",
    "remove = string.punctuation\n",
    "remove = remove.replace(\"-\", \"\").replace(\",\", \"\") # don't remove hyphens\n",
    "pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import logging\n",
    "from retry import retry\n",
    "logging.basicConfig()\n",
    "\n",
    "# Calculate the delay based on your rate limit\n",
    "rate_limit_per_minute = 5000.0\n",
    "delay_60 = 60.0 / 60\n",
    "delay_full = 60.0 / rate_limit_per_minute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d4601-a181-4fc0-9ff0-efb3f0d8b0ff",
   "metadata": {},
   "source": [
    "## General Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e40c53c4-f26c-4dc7-babe-39cb77569822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"mfrc\"\n",
    "mode = \"full\"\n",
    "folder = \"../data/preprocessed/\"\n",
    "path = folder + data + \"_sample_\" + mode + \".csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99e6b8-baea-42dc-8467-dafcd97472b8",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8410c23-d642-40ea-bb96-b6392c4aeec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chatGPT parameters\n",
    "HOST = 'localhost:5000'\n",
    "URI = f'http://{HOST}/api/v1/generate'\n",
    "\n",
    "@retry(delay=5)\n",
    "def run(prompt, verbose=0, slow_down=0.001):\n",
    "    request = {\n",
    "        'prompt': prompt,\n",
    "        'max_new_tokens': 150,\n",
    "        'mode' : 'instruct',\n",
    "\n",
    "        # Generation params. If 'preset' is set to different than 'None', the values\n",
    "        # in presets/preset-name.yaml are used instead of the individual numbers.\n",
    "        'preset': 'None',\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.01,\n",
    "        'top_p': 0.14,\n",
    "        'typical_p': 1,\n",
    "        'epsilon_cutoff': 0,  # In units of 1e-4\n",
    "        'eta_cutoff': 0,  # In units of 1e-4\n",
    "        'tfs': 1,\n",
    "        'top_a': 0,\n",
    "        'repetition_penalty': 1.17,\n",
    "        'repetition_penalty_range': 0,\n",
    "        'encoder_repetition_penalty': 1,\n",
    "        'top_k': 49,\n",
    "        'min_length': 0,\n",
    "        'no_repeat_ngram_size': 0,\n",
    "        'num_beams': 1,\n",
    "        'penalty_alpha': 0,\n",
    "        'length_penalty': 1,\n",
    "        'early_stopping': False,\n",
    "        'mirostat_mode': 0,\n",
    "        'mirostat_tau': 5,\n",
    "        'mirostat_eta': 0.1,\n",
    "        # 'instruction_template': \"Instruct-Alpaca\",\n",
    "\n",
    "        'seed': -1,\n",
    "        'add_bos_token': True,\n",
    "        'truncation_length': 2048,\n",
    "        'ban_eos_token': False,\n",
    "        'skip_special_tokens': True,\n",
    "        'stopping_strings': []\n",
    "    }\n",
    "\n",
    "    response = requests.post(URI, json=request)\n",
    "\n",
    "    if response.status_code == 200 and verbose == 1:\n",
    "        result = response.json()['results'][0]['text']\n",
    "        print(prompt + result)\n",
    "    time.sleep(slow_down)\n",
    "    return response\n",
    "\n",
    "def model_api(request):\n",
    "    response = requests.post(f'http://{HOST}/api/v1/model', json=request)\n",
    "    return response.json()\n",
    "\n",
    "def model_info():\n",
    "    response = model_api({'action': 'info'})\n",
    "    print_basic_model_info(response)\n",
    "\n",
    "def print_basic_model_info(response):\n",
    "    basic_settings = ['truncation_length', 'instruction_template']\n",
    "    print(\"Model: \", response['result']['model_name'])\n",
    "    print(\"Lora(s): \", response['result']['lora_names'])\n",
    "    for setting in basic_settings:\n",
    "        print(setting, \"=\", response['result']['shared.settings'][setting])\n",
    "\n",
    "def separate_labels(df, cols):\n",
    "    def _set_labels(row):\n",
    "        for label in row[\"annotations\"].split(\",\"):\n",
    "            if label.strip() in cols:\n",
    "                row[label.strip()] = 1\n",
    "        return row\n",
    "\n",
    "    # removing texts with no annotations\n",
    "    df = df[df.annotations != ''].reset_index(drop=True)\n",
    "    df = df[~ pd.isna(df.annotations)].reset_index(drop=True)\n",
    "    for label in cols:\n",
    "        df[label] = 0\n",
    "    df = df.apply(_set_labels, axis=1).drop([\"annotations\"], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9384f743-4d22-4464-ae81-41cc3d89e742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  TheBloke_Luna-AI-Llama2-Uncensored-GPTQ_gptq-4bit-32g-actorder_True\n",
      "Lora(s):  []\n",
      "truncation_length = 2048\n",
      "instruction_template = None\n"
     ]
    }
   ],
   "source": [
    "model_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75469f4-5b5f-4463-bef5-1f325d533c7d",
   "metadata": {},
   "source": [
    "## Load Data and Create Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd822b6e-7bbf-4920-8fe5-678d5118e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEXT_1 = \"USER: These are definitions of moral sentiments: \"\\\n",
    "\"\\\"care\\\" if a text is about avoiding emotional and physical damage to another individual, \\\"equality\\\" if a text is about equal treatment and equal outcome for individuals, \"\\\n",
    "\"\\\"proportionality\\\" if a text is about individuals getting rewarded in proportion to their merit or contribution, \\\"loyalty\\\" if a text is about cooperating with ingroups and competing with outgroups, \"\\\n",
    "\"\\\"authority\\\" if a text is about deference toward legitimate authorities and the defense of traditions, all of which are seen as providing stability and fending off chaos, \"\\\n",
    "\"\\\"purity\\\" if a text is about avoiding bodily and spiritual contamination and degradation, \\\"thin-morality\\\" if a text has a moral sentiment but cannot be categorized as either of the above, \"\\\n",
    "\"\\\"none\\\" if no moral sentiment is expressed in the text.\"\\\n",
    "\"\\n\\nBased solely on these definitions, name all moral sentiments that are directly expressed in the following text:\\n\"\\\n",
    "\"\\\"\\\"\\\"\"\n",
    "\n",
    "PROMPT_TEXT_2 = \"\\\"\\\"\\\"\\n\\nReturn a comma separated list of all moral sentiments that were expressed in the text.\\n\\nASSISTANT:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6caedb6f-8cf5-40e9-a03a-cf8b9682b54b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2983, 9)\n",
      "33\n",
      "USER: These are definitions of moral sentiments: \"care\" if a text is about avoiding emotional and physical damage to another individual, \"equality\" if a text is about equal treatment and equal outcome for individuals, \"proportionality\" if a text is about individuals getting rewarded in proportion to their merit or contribution, \"loyalty\" if a text is about cooperating with ingroups and competing with outgroups, \"authority\" if a text is about deference toward legitimate authorities and the defense of traditions, all of which are seen as providing stability and fending off chaos, \"purity\" if a text is about avoiding bodily and spiritual contamination and degradation, \"thin-morality\" if a text has a moral sentiment but cannot be categorized as either of the above, \"none\" if no moral sentiment is expressed in the text.\n",
      "\n",
      "Based solely on these definitions, name all moral sentiments that are directly expressed in the following text:\n",
      "\"\"\"Was just browsing r/Politics, noticed [this tired old trope](https://www.reddit.com/r/politics/comments/6goce7/bernie_sanders_says_labour_party_shows_the_way_to/dirtr7d/) rearing it's ugly head. DAE Macron is actually left for American politics?\"\"\"\n",
      "\n",
      "Return a comma separated list of all moral sentiments that were expressed in the text.\n",
      "\n",
      "ASSISTANT:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create prompts\n",
    "df = pd.read_csv(path)\n",
    "print(df.shape)\n",
    "print(round(df.text.str.split(\"\\\\s+\").str.len().mean()))\n",
    "prompts = [PROMPT_TEXT_1 + x + PROMPT_TEXT_2 for x in df.text]\n",
    "print(prompts[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d9776-c62e-448d-a7a7-f4cab94b15dd",
   "metadata": {},
   "source": [
    "## Test Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5ece223-9a5b-40b4-9f79-542e14540337",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: These are definitions of moral sentiments: \"care\" if a text is about avoiding emotional and physical damage to another individual, \"equality\" if a text is about equal treatment and equal outcome for individuals, \"proportionality\" if a text is about individuals getting rewarded in proportion to their merit or contribution, \"loyalty\" if a text is about cooperating with ingroups and competing with outgroups, \"authority\" if a text is about deference toward legitimate authorities and the defense of traditions, all of which are seen as providing stability and fending off chaos, \"purity\" if a text is about avoiding bodily and spiritual contamination and degradation, \"thin-morality\" if a text has a moral sentiment but cannot be categorized as either of the above, \"none\" if no moral sentiment is expressed in the text.\n",
      "\n",
      "Based solely on these definitions, name all moral sentiments that are directly expressed in the following text:\n",
      "\"\"\"Yes, it's understandable for the victims' loved ones to be in immense pain, and it's a genuine irreparable tragedy. But that's also why we don't let the families of victims decide what happens to the offenders if we want to have a nation of impartial justice and law.\"\"\"\n",
      "\n",
      "Return a comma separated list of all moral sentiments that were expressed in the text.\n",
      "\n",
      "ASSISTANT:\n",
      "care, loyalty\n"
     ]
    }
   ],
   "source": [
    "test_prompt = prompts[2]\n",
    "answer_test = run(test_prompt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce67b9f-4dbc-4ffb-9e66-bbf82f6881f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a5c57ca5-e53a-4bef-bce8-db79611d7536",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt_style' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m cols \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     17\u001b[0m df_preds \u001b[38;5;241m=\u001b[39m separate_labels(df_responses, cols)\n\u001b[1;32m---> 18\u001b[0m df_preds\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../results/predictions/llama2_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m data \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_labels_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m mode \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mprompt_style\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prompt_style' is not defined"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for i, prompt in enumerate(prompts):\n",
    "    APIresponse = run(prompt, 0, delay_full)\n",
    "    response = APIresponse.json()[\"results\"][0][\"text\"]\n",
    "    responses.append(response)\n",
    "\n",
    "# define categories to be found in llama output\n",
    "foundations = [\"care\", \"equality\", \"proportionality\", \"loyalty\", \"authority\", \"purity\", \"thin-morality\"]\n",
    "# clean gpt outputs (for predictions that have imprecise wording, e.g., punctuation)\n",
    "responses_cleaned = [re.sub(pattern, \"\", x.lower()) for x in responses]\n",
    "# responses_cleaned = [x if \"no moral sentiments\" not in x.lower() else \"non-moral\" for x in responses_cleaned]\n",
    "responsesToFoundations = [list(set([y for y in foundations if y in x])) for x in responses_cleaned] #find foundation names in cleaned strings\n",
    "responsesToFoundations = [[\"non-moral\"] if not x else x for x in responsesToFoundations] # no foundation = non-moral\n",
    "responsesToFoundations = [\",\".join(x) for x in responsesToFoundations] #reformat for further processing\n",
    "\n",
    "new_dic = {}\n",
    "new_dic[\"text\"] = df.text.tolist()\n",
    "new_dic[\"annotations\"] = responsesToFoundations\n",
    "df_responses = pd.DataFrame(new_dic)\n",
    "\n",
    "cols = df.columns[1:].tolist()\n",
    "df_preds = separate_labels(df_responses, cols) # format to final dataset\n",
    "df_preds.to_csv(\"../results/predictions/llama2_\" + data + \"_labels_\" + mode + \".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
