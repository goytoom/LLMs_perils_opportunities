{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ecbeb22-6508-4d3b-b875-0a0f6f63dd4a",
   "metadata": {},
   "source": [
    "This codebook prepares the data for the text annotation task:\n",
    "-   Rename output labels (here in line with moral foundations theory; e.g., if one wants to cluster values into hierarchies)\n",
    "    - Here we choose to combine vices and virtues of the same foundation to simplify interpretation (as it is done in most cases)\n",
    "-   Splits the total MFRC into a train/fine-tune part (for BERT) and an evaluation/groundtruth part (to test BERT and ChatGPT performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf60be-cd1e-4981-b32c-8edb1d537e88",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c751b5e-4f85-443d-8bb7-31ec2839998a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabdurah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from html import unescape\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tokenization\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "import pickle as pkl\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae91339-9b50-49ee-b028-a266c90e5678",
   "metadata": {},
   "source": [
    "## Dicts (auxiliary) \n",
    " - Allows to cluster our labels according to whatever theoretical consideration (e.g., usually vices and virtues are not separated)\n",
    " - E.g., betrayal and loyalty or subversion and authority are just different sides of the same moral value (positive/negative)\n",
    " - Here, we utilize \"full\", which uses moral values in line with the MFQ2 (care, equality, proportionality, loyalty, authority, purity, + thin-morality & non-moral), in line with the authors of the MFRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "843146fc-3f98-4f9c-b204-94f885592476",
   "metadata": {},
   "outputs": [],
   "source": [
    "foundations = {\"mfrc\":  {\n",
    "                    \"complete\": [\"care\", \"harm\", \"equality\", \"proportionality\", \"loyalty\", \"betrayal\", \"authority\", \"subversion\", \"purity\", \"degradation\", \"thin morality\", \"non-moral\"],\n",
    "                    \"binding\": [\"individual\", \"binding\", \"proportionality\", \"thin morality\", \"non-moral\"], \n",
    "                    \"moral\": [\"moral\", \"thin morality\", \"non-moral\"],\n",
    "                    \"full\": [\"care\", \"proportionality\", \"loyalty\", \"authority\", \"purity\", \"equality\", \"thin morality\", \"non-moral\"],\n",
    "               }\n",
    "              }\n",
    "\n",
    "foundations_dict = {\n",
    "\n",
    "                    \"complete\": {\"harm\": \"harm\", \"care\": \"care\", \"degradation\": \"degradation\", \n",
    "                                        \"purity\": \"purity\", \"betrayal\": \"betrayal\", \"loyalty\": \"loyalty\", \n",
    "                                        \"subversion\": \"subversion\", \"authority\": \"authority\",\n",
    "                                        \"cheating\": \"cheating\", \"fairness\": \"fairness\",  \"equality\": \"equality\",\n",
    "                                        \"non-moral\": \"non-moral\", \"nm\": \"non-moral\", \"thin morality\": \"thin morality\", \"proportionality\": \"proportionality\"},\n",
    "    \n",
    "                    \"binding\": {\"harm\": \"individual\", \"care\": \"individual\", \"degradation\": \"binding\", \n",
    "                                \"purity\": \"binding\", \"betrayal\": \"binding\", \"loyalty\": \"binding\", \n",
    "                                \"subversion\": \"binding\", \"authority\": \"binding\",\n",
    "                                \"cheating\": \"individual\", \"fairness\": \"individual\",  \"equality\": \"individualizing\",\n",
    "                                \"non-moral\": \"non-moral\", \"nm\": \"non-moral\", \"proportionality\": \"proportionality\", \"thin morality\": \"thin morality\"},\n",
    "                    \n",
    "                    \"moral\": {\"harm\": \"moral\", \"care\": \"moral\", \"degradation\": \"moral\", \n",
    "                                    \"purity\": \"moral\", \"betrayal\": \"moral\", \"loyalty\": \"moral\", \n",
    "                                    \"subversion\": \"moral\", \"authority\": \"moral\",\n",
    "                                    \"cheating\": \"moral\", \"fairness\": \"moral\",  \"equality\": \"moral\",\n",
    "                                    \"non-moral\": \"non-moral\", \"nm\": \"non-moral\", \"thin morality\": \"thin morality\", \"proportionality\": \"moral\"},\n",
    "                   \n",
    "                    \"full\": {\"harm\": \"care\", \"care\": \"care\", \"degradation\": \"purity\", \n",
    "                                        \"purity\": \"purity\", \"betrayal\": \"loyalty\", \"loyalty\": \"loyalty\", \n",
    "                                        \"subversion\": \"authority\", \"authority\": \"authority\",\n",
    "                                        \"cheating\": \"fairness\", \"fairness\": \"fairness\", \"equality\": \"equality\",\n",
    "                                        \"non-moral\": \"non-moral\", \"nm\": \"non-moral\", \"thin morality\": \"thin morality\", \"proportionality\": \"proportionality\"},\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68074f9b-3ca2-4654-a797-85d2ef07d4c0",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8775de68-eb8e-4932-a04f-18fac7a124ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset(data_file, bert_layer, mode, corp):\n",
    "    df = pd.read_csv(data_file)\n",
    "    X, y = get_needed_fields(df, cols = foundations[corp][mode])\n",
    "    X, y = pre_process_text(X, y, tokenizer)\n",
    "    y=np.array(y) \n",
    "    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(X, y, np.arange(len(X)), test_size=0.2, shuffle=True, random_state=0)\n",
    "    X_train = bert_encode(X_train, tokenizer)\n",
    "    X_test=bert_encode(X_test,tokenizer)\n",
    "    y_train = tf.convert_to_tensor(y_train, dtype=np.float32)\n",
    "    y_test = tf.convert_to_tensor(y_test, dtype=np.float32)\n",
    "    print(len(X_train[0]), len(y_train))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, idx_test\n",
    "\n",
    "def construct_dataset_full(data_file, bert_layer, mode, corp):\n",
    "    df = pd.read_csv(data_file)\n",
    "    X, y = get_needed_fields(df, cols = foundations[corp][mode])\n",
    "    X, y = pre_process_text(X, y, tokenizer)\n",
    "    y=np.array(y)\n",
    "    X = bert_encode(X, tokenizer)\n",
    "    y = tf.convert_to_tensor(y, dtype=np.float32)\n",
    "    print(len(X[0]), len(y))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def get_needed_fields(df, cols = [\"individual\", \"binding\", \"non-moral\"]):\n",
    "    \n",
    "    X = list(df[\"text\"])\n",
    "    y = df[foundations[corp][mode]].values\n",
    "    # Y_encoded = encoder.fit_transform(df.annotation)\n",
    "    # y = to_categorical(Y_encoded)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=256):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        text = text[:max_len - 2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "\n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "\n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "\n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "\n",
    "def pre_process_text(X, y, tokenizer):\n",
    "    tokenized = [tokenizer.tokenize(x) for x in X]\n",
    "    results = []\n",
    "    labels = []\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "\n",
    "    ## remove stop words and make lower case\n",
    "    # maybe adjust this\n",
    "    en_stopwords = stopwords.words('english')\n",
    "    for i, text in enumerate(tokenized):\n",
    "        out = [token for token in text if (token not in en_stopwords) and (token not in symbols)\n",
    "               and (not token.startswith(\"@\")\n",
    "                    and (not token.startswith(\"http\")))]\n",
    "        if len(out) >= 5:               # remove tweets that are too short after preprocessing\n",
    "            results.append(out)\n",
    "            labels.append(y[i])\n",
    "    return results, labels\n",
    "\n",
    "def pre_process_df(df, tokenizer):\n",
    "    df[\"tokenized\"] = df.text.apply(lambda x: tokenizer.tokenize(x))\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "\n",
    "    ## remove stop words and make lower case\n",
    "    # maybe adjust this\n",
    "    en_stopwords = stopwords.words('english')\n",
    "    df.tokenized = df.tokenized.apply(lambda x: [token for token in x if (token not in en_stopwords) and (token not in symbols)\n",
    "               and (not token.startswith(\"@\")\n",
    "                    and (not token.startswith(\"http\")))])\n",
    "    \n",
    "    df = df[df.tokenized.str.len() >=5].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "################ Additional Functions\n",
    "\n",
    "def _set_majority_vote(row):\n",
    "    for label in foundations[corp][mode]:\n",
    "        if row[label] >= row['annotation']:\n",
    "            row[label] = 1\n",
    "        else:\n",
    "            row[label] = 0\n",
    "    return row\n",
    "\n",
    "def separate_labels(df):\n",
    "    def _set_labels(row):\n",
    "        for label in row[\"annotation\"].split(\",\"):\n",
    "            row[label] = 1\n",
    "        return row\n",
    "\n",
    "    # removing texts with no annotations\n",
    "    df = df[df.annotation != ''].reset_index(drop=True)\n",
    "    df = df[~ pd.isna(df.annotation)]\n",
    "    for label in foundations[corp][mode]:\n",
    "        df[label] = 0\n",
    "    df = df.apply(_set_labels, axis=1)\n",
    "    return df\n",
    "\n",
    "def calculate_majortiy_labels(df, corp):\n",
    "    \"\"\"\n",
    "    calculates majority vote for the moral foundations annotations for each text\n",
    "    Returns dataset with majority labels\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if corp == \"mfrc\":\n",
    "        agg_dict = {\"annotation\": \"count\"} #, 'bucket': \"first\", 'subreddit': 'first'}\n",
    "    else:\n",
    "        agg_dict = {\"annotation\": \"count\"}\n",
    "    for label in foundations[corp][mode]:\n",
    "        agg_dict[label] = \"sum\"\n",
    "\n",
    "    df = df.groupby([\"text\"], as_index=False).agg(agg_dict).reset_index(drop=True)\n",
    "    df['annotation'] = df['annotation'].div(2)\n",
    "    df = df.apply(_set_majority_vote, axis=1)\n",
    "    df[\"sum\"] = df[foundations[corp][mode]].sum(axis=1)\n",
    "    df = df[df[\"sum\"] != 0]\n",
    "    df = df.drop(columns=[\"sum\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# load bert model (base)\n",
    "module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b681846c-2e89-494f-86b9-103678490f47",
   "metadata": {},
   "source": [
    "## General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07e851b1-27d6-48a8-96be-4ec67311c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = \"mfrc\" # change when using different corpora\n",
    "mode = \"full\" # change when using different clustering of sentiment/target variable (here moral values)\n",
    "\n",
    "main_path = \"../data/preprocessed/\"\n",
    "eval_path = main_path + corp + \"_eval_\" + mode + \".csv\"\n",
    "sample_meta_path = main_path + corp + \"_meta_sample_\" + mode + \".csv\"\n",
    "sample_path = main_path + corp + \"_sample_\" + mode + \".csv\"\n",
    "final_path = main_path + corp + \"_cleaned_\" + mode + \".csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8af035a-215b-4268-ab9e-873e74429cef",
   "metadata": {},
   "source": [
    "## Process data (format for nlp models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c9d2610-6099-4b4e-990a-e7ed6716c86b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/raw/final_mfrc_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m pre_process_df(df_raw, tokenizer)\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized\u001b[39m\u001b[38;5;124m\"\u001b[39m], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m df\u001b[38;5;241m.\u001b[39mannotation \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mannotation\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mreplace(foundations_dict[mode], regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# format for evaluation against predictions (need this for annotator analyses)    \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "df_raw = pd.read_csv(\"../data/raw/final_mfrc_data.csv\") # MFRC data (current version, might change in the future)\n",
    "df_raw = pre_process_df(df_raw, tokenizer).drop([\"tokenized\"], axis = 1)\n",
    "df = df_raw.drop_duplicates().reset_index(drop=True)\n",
    "df.annotation = df.annotation.str.lower().replace(foundations_dict[mode], regex=True)\n",
    "\n",
    "# format for evaluation against predictions (need this for annotator demographics analyses)    \n",
    "df_eval = df[[\"text\", \"annotator\", \"annotation\"]]\n",
    "df_eval.loc[:, \"annotation\"] = df_eval.annotation.apply(lambda x: x.split(','))\n",
    "df_eval = df_eval.explode(\"annotation\").reset_index(drop=True)\n",
    "df_eval = df_eval.drop_duplicates().reset_index(drop=True)\n",
    "df_eval.to_csv(eval_path, index = False) # --> save groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "2f0242f4-65e1-43c3-bccf-a967de13a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = separate_labels(df)\n",
    "df_final = calculate_majortiy_labels(df_final, corp)\n",
    "df_final = df_final.drop([\"annotation\"], axis = 1)\n",
    "df_final.to_csv(final_path, index = False) # full MFRC data: All tweets x all moral foundations ratings (each as binary variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba317a-c6f3-4192-8e0a-b3178595cf90",
   "metadata": {},
   "source": [
    "## Create test/train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86e54e3c-5ddd-41f8-9fdd-7d64bf0435f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11928 11928\n",
      "14911 14911\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, idx_test = construct_dataset(final_path, bert_layer=bert_layer, mode=mode, corp=corp) #save test IDs\n",
    "with open(\"../data/train_test/\" + corp + \"_train_\" + mode + \".pkl\",\"wb\") as f:\n",
    "    pkl.dump([X_train, y_train], f)\n",
    "with open(\"../data/train_test/\" + corp + \"_test_\" + mode + \".pkl\",\"wb\") as f:\n",
    "    pkl.dump([X_test, y_test], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def9bdc-725a-452f-afef-fde82689b7d2",
   "metadata": {},
   "source": [
    "Get groundtruth data for chatGPT comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "5dc4f334-a375-44ed-9d25-10990cb0c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the metainformation for the sample that will be compared with chatGPT\n",
    "df_eval = pd.read_csv(eval_path)    # get data with explicit annotator information\n",
    "df_final = pd.read_csv(final_path)  # get data that the train/test split was performed on\n",
    "\n",
    "df_sample = df_final.iloc[idx_test]   # get the posts for the test sample\n",
    "df_sample_meta = df_eval.loc[df_eval.text.isin(df_sample.text)].reset_index(drop=True) # find test sample in annotator data\n",
    "df_sample_meta.to_csv(sample_meta_path, index=False) # save annotator information about sample\n",
    "df_sample.to_csv(sample_path, index=False) # save sample (groundtruth for performance calculations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
