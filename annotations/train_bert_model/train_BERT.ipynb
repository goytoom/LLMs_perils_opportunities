{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f0e2c4-a3e3-49de-be1f-12c94af0a6b3",
   "metadata": {},
   "source": [
    "This codebook trains and fine-tunes a BERT model to predict moral sentiment  \n",
    "This can be very slow depending on hardware.  \n",
    "We used a v100, 32GB of RAM, 8 CPUS  \n",
    "However, this code should be able to run on a system with 16GB of RAM, a dedicated GPU (we tested it on a RTX 2070s), and a 6-core CPU (e.g., Ryzen 5 3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2371f9-191a-40a0-8330-e1f8b018990a",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ccec441-464b-4b1e-abf6-72d9c58083af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabdurah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import load_model \n",
    "from keras.metrics import Precision, Recall\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import tokenization\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "foundations = {\"mfrc\":  {\n",
    "                    \"complete\": [\"care\", \"harm\", \"equality\", \"proportionality\", \"loyalty\", \"betrayal\", \"authority\", \"subversion\", \"purity\", \"degradation\", \"thin morality\", \"non-moral\"],\n",
    "                    \"binding\": [\"individual\", \"binding\", \"proportionality\", \"thin morality\", \"non-moral\"], \n",
    "                    \"moral\": [\"moral\", \"thin morality\", \"non-moral\"],\n",
    "                    \"full\": [\"care\", \"proportionality\", \"loyalty\", \"authority\", \"purity\", \"equality\", \"thin morality\", \"non-moral\"]\n",
    "               }\n",
    "              }\n",
    "classes = {\"mfrc\": {\"full\": 8, \"moral\": 3, \"binding\": 5, \"complete\": 12}}\n",
    "activation = {\"full\": \"sigmoid\", \"moral\": \"sigmoid\", \"binding\": \"sigmoid\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c002d5d-2d54-4ab6-98a6-2abdd6620dc1",
   "metadata": {},
   "source": [
    "## Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "628b61f4-47da-4c29-866d-0a2ccec0fa88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512, classes = 5, activation = \"sigmoid\"):\n",
    "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    outputs= bert_layer(dict(input_word_ids=input_word_ids,\n",
    "    input_mask=input_mask,\n",
    "    input_type_ids=segment_ids))\n",
    "\n",
    "    sequence_output=outputs[\"sequence_output\"]\n",
    "\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = tf.keras.layers.Dense(classes, activation=activation)(clf_output)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                  loss='binary_crossentropy', metrics=[Precision(), Recall()])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def get_binary(_y, threshold):\n",
    "    y = _y.copy()\n",
    "    y[y >= threshold] = 1\n",
    "    y[y < threshold] = 0\n",
    "    return y\n",
    "\n",
    "def F1Measure(y_true, y_pred, threshold=0.5):\n",
    "    y_binary = get_binary(y_pred, threshold)\n",
    "    score = f1_score(y_true, y_binary, average = \"macro\")   \n",
    "\n",
    "    return score\n",
    "\n",
    "def train(mode, bert_layer, corp):\n",
    "    \n",
    "    model = build_model(bert_layer, max_len=256, classes = classes[corp][mode], activation = activation[mode])\n",
    "\n",
    "    with open(\"../data/train_test/\" + corp + \"_train_\" + mode + \".pkl\", \"rb\") as f:\n",
    "        X_train, y_train = pkl.load(f)\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint('../models/' + corp + \"_\" + training + \"_\" + mode + '.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "    print(\"start training\")\n",
    "    t = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.1,\n",
    "        epochs=200,\n",
    "        callbacks=[checkpoint, earlystopping],\n",
    "        batch_size=32, #32 works best so far\n",
    "        verbose=1)\n",
    "    print(\"Saving the model\")\n",
    "\n",
    "def crossVal(mode, threshold):\n",
    "       \n",
    "    with open(\"../data/train_test/\" + corp + \"_train_\" + mode + \".pkl\", \"rb\") as f:\n",
    "        X, y = pkl.load(f)\n",
    "\n",
    "    model_file = '../models/' + corp + '_' + training + \"_\" + mode + '_cv.h5'\n",
    "\n",
    "    print(\"Start Cross-Validation\")\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "    cvscores = []\n",
    "    for train, test in kfold.split(X[0], reverse_onehot(y)): #potentially use CV folds as predictions to evaluate against chatGPT\n",
    "        tf.keras.backend.clear_session() # remove any past model from session\n",
    "        if os.path.isfile(model_file): # remove saved models from checkpoint\n",
    "            os.remove(model_file)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
    "        model = build_model(bert_layer, max_len=256, classes = classes[corp][mode], activation = activation[mode])\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(model_file, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "        \n",
    "        X_train_cv = (X[0][train], X[1][train], X[2][train])\n",
    "        y_train_cv = tf.gather(y, train)\n",
    "        X_test_cv = (X[0][test], X[1][test], X[2][test])\n",
    "        y_test_cv = tf.gather(y, test)\n",
    "        t = model.fit(\n",
    "            X_train_cv, y_train_cv,\n",
    "            validation_data = (X_test_cv, y_test_cv),\n",
    "            epochs=200,\n",
    "            callbacks=[checkpoint, earlystopping],\n",
    "            batch_size=32, #32 works best so far\n",
    "            verbose=1)\n",
    "\n",
    "        #load best model from training\n",
    "        tf.keras.backend.clear_session() \n",
    "        model = load_model(model_file, compile=True, custom_objects={\"KerasLayer\": bert_layer})\n",
    "        y_pred_val = model.predict(X_test_cv)\n",
    "        score = F1Measure(y_test_cv, y_pred_val, threshold)\n",
    "        cvscores.append(score * 100)\n",
    "        print(\"%s: %.2f%%\" % (\"F1-Score (macro average)\", score*100))\n",
    "        \n",
    "        score2 = f1_score(y_test_cv, get_binary(y_pred_val, threshold), average=None)\n",
    "        print(score2.round(3)*100)        \n",
    "        \n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "\n",
    "def reverse_onehot(onehot_data):\n",
    "    # onehot_data assumed to be channel last\n",
    "    data_copy = np.zeros(onehot_data.shape[:-1])\n",
    "    for c in range(onehot_data.shape[-1]):\n",
    "        img_c = onehot_data[..., c]\n",
    "        data_copy[img_c == 1] = c\n",
    "    return data_copy\n",
    "    \n",
    "module_url = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/2\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea78f415-c96f-4052-a9ef-e8d556f89c70",
   "metadata": {},
   "source": [
    "## General Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d43a9276-36a2-4efb-b307-4adbc41b62d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose MFRC as corpus (can be changed to run on other corpora as necessary)\n",
    "# choose to run on full MFT dimensions (see prepare_data for different ways of categorizing the moral values)\n",
    "# Choose between training=eval for determining train/validation accuracy (e.g., when optimizing parameters) and training=normal to train the model\n",
    "\n",
    "corp = \"mfrc\"\n",
    "mode = \"full\"\n",
    "training = \"eval\"\n",
    "threshold = 0.3 #change this value when using eval (decision rule for classification; can impact accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbbbdbd-5b77-4fc1-8ac8-7fd1723ffb2e",
   "metadata": {},
   "source": [
    "## Train/Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dea8c691-cce9-4e34-820a-a011e9e109d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_mask (InputLayer)        [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " input_word_ids (InputLayer)    [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " keras_layer_1 (KerasLayer)     {'pooled_output': (  17488641    ['input_mask[0][0]',             \n",
      "                                None, 256),                       'segment_ids[0][0]',            \n",
      "                                 'sequence_output':               'input_word_ids[0][0]']         \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 256, 256),                                               \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256),                                                \n",
      "                                 (None, 256, 256)],                                               \n",
      "                                 'default': (None,                                                \n",
      "                                256)}                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 256)         0           ['keras_layer_1[1][14]']         \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 8)            2056        ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,490,697\n",
      "Trainable params: 17,490,696\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "start training\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-04 00:23:05.176826: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype int32 and shape [?,256]\n",
      "\t [[{{node Placeholder_1}}]]\n",
      "2023-09-04 00:23:05.176885: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_2' with dtype int32 and shape [?,256]\n",
      "\t [[{{node Placeholder_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - ETA: 0s - loss: 0.2485 - precision_2: 0.7322 - recall_2: 0.5445\n",
      "Epoch 1: val_loss improved from inf to 0.20754, saving model to ../models/mfrc_normal_full2.h5\n",
      "336/336 [==============================] - 73s 163ms/step - loss: 0.2485 - precision_2: 0.7322 - recall_2: 0.5445 - val_loss: 0.2075 - val_precision_2: 0.8349 - val_recall_2: 0.5727\n",
      "Epoch 2/200\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.2098 - precision_2: 0.8037 - recall_2: 0.5799\n",
      "Epoch 2: val_loss improved from 0.20754 to 0.19980, saving model to ../models/mfrc_normal_full2.h5\n",
      "336/336 [==============================] - 41s 120ms/step - loss: 0.2098 - precision_2: 0.8037 - recall_2: 0.5799 - val_loss: 0.1998 - val_precision_2: 0.8248 - val_recall_2: 0.5773\n",
      "Epoch 3/200\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.1930 - precision_2: 0.8127 - recall_2: 0.6108\n",
      "Epoch 3: val_loss improved from 0.19980 to 0.18446, saving model to ../models/mfrc_normal_full2.h5\n",
      "336/336 [==============================] - 39s 116ms/step - loss: 0.1930 - precision_2: 0.8127 - recall_2: 0.6108 - val_loss: 0.1845 - val_precision_2: 0.8188 - val_recall_2: 0.6461\n",
      "Epoch 4/200\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.1802 - precision_2: 0.8249 - recall_2: 0.6425\n",
      "Epoch 4: val_loss improved from 0.18446 to 0.18053, saving model to ../models/mfrc_normal_full2.h5\n",
      "336/336 [==============================] - 38s 114ms/step - loss: 0.1802 - precision_2: 0.8249 - recall_2: 0.6425 - val_loss: 0.1805 - val_precision_2: 0.8010 - val_recall_2: 0.6539\n",
      "Epoch 5/200\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.1701 - precision_2: 0.8277 - recall_2: 0.6622\n",
      "Epoch 5: val_loss improved from 0.18053 to 0.17892, saving model to ../models/mfrc_normal_full2.h5\n",
      "336/336 [==============================] - 37s 111ms/step - loss: 0.1701 - precision_2: 0.8277 - recall_2: 0.6622 - val_loss: 0.1789 - val_precision_2: 0.8013 - val_recall_2: 0.6648\n",
      "Epoch 6/200\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.1610 - precision_2: 0.8369 - recall_2: 0.6850\n",
      "Epoch 6: val_loss improved from 0.17892 to 0.17492, saving model to ../models/mfrc_normal_full2.h5\n",
      "336/336 [==============================] - 38s 113ms/step - loss: 0.1610 - precision_2: 0.8369 - recall_2: 0.6850 - val_loss: 0.1749 - val_precision_2: 0.8241 - val_recall_2: 0.6773\n",
      "Epoch 7/200\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.1522 - precision_2: 0.8448 - recall_2: 0.7010\n",
      "Epoch 7: val_loss did not improve from 0.17492\n",
      "336/336 [==============================] - 36s 107ms/step - loss: 0.1522 - precision_2: 0.8448 - recall_2: 0.7010 - val_loss: 0.1818 - val_precision_2: 0.7932 - val_recall_2: 0.6742\n",
      "Epoch 8/200\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.1431 - precision_2: 0.8551 - recall_2: 0.7213\n",
      "Epoch 8: val_loss did not improve from 0.17492\n",
      "336/336 [==============================] - 31s 93ms/step - loss: 0.1431 - precision_2: 0.8551 - recall_2: 0.7213 - val_loss: 0.1823 - val_precision_2: 0.8011 - val_recall_2: 0.6766\n",
      "Epoch 9/200\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.1342 - precision_2: 0.8632 - recall_2: 0.7403\n",
      "Epoch 9: val_loss did not improve from 0.17492\n",
      "336/336 [==============================] - 22s 65ms/step - loss: 0.1342 - precision_2: 0.8632 - recall_2: 0.7403 - val_loss: 0.1833 - val_precision_2: 0.7956 - val_recall_2: 0.6844\n",
      "Epoch 10/200\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.1259 - precision_2: 0.8746 - recall_2: 0.7610\n",
      "Epoch 10: val_loss did not improve from 0.17492\n",
      "336/336 [==============================] - 21s 62ms/step - loss: 0.1259 - precision_2: 0.8746 - recall_2: 0.7610 - val_loss: 0.1846 - val_precision_2: 0.7991 - val_recall_2: 0.7148\n",
      "Epoch 11/200\n",
      "336/336 [==============================] - ETA: 0s - loss: 0.1178 - precision_2: 0.8832 - recall_2: 0.7776\n",
      "Epoch 11: val_loss did not improve from 0.17492\n",
      "336/336 [==============================] - 21s 64ms/step - loss: 0.1178 - precision_2: 0.8832 - recall_2: 0.7776 - val_loss: 0.1921 - val_precision_2: 0.7838 - val_recall_2: 0.7023\n",
      "Epoch 11: early stopping\n",
      "Saving the model\n"
     ]
    }
   ],
   "source": [
    "if training == \"eval\": # determine best model using CV\n",
    "    crossVal(mode, threshold)\n",
    "elif training == \"normal\": # regular training for test sample (against chatGPT)\n",
    "    train(mode, bert_layer, corp)\n",
    "else:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
